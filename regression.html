<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Regression</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">

</head>
<body>

   <!-- Elegant Animated Background -->
   <div class="food-background"></div>
   <div class="light-overlay"></div>

    <!-- üîπ Hamburger Menu Icon -->
    <div class="menu-icon" onclick="toggleMenu()">‚ò∞</div>
    <div class="navbar">
        <a href="index.html">Home</a>
        <a href="introduction.html">Introduction</a>
        <a href="data_prep.html">Data Prep</a>
        <a href="eda.html">EDA</a>        
        <a href="pca.html">PCA</a>
        <a href="clustering.html">Clustering</a>
        <a href="arm.html">ARM</a>
        <a href="decision_trees.html">Decision Trees</a>
        <a href="naive_bayes.html">Naive Bayes</a>
        <a href="svm.html">SVM</a>
        <a href="regression.html"class="active">Regression</a>
        <a href="randomforest.html">Random Forest</a>
        <a href="conclusions.html">Conclusions</a>
        <a href="about_me.html">About Me</a>
    </div>

    <div class="content-section">
        <h1 id="typing-title"></h1>
        <div class="content-wrapper">

            <!-- (a) Linear Regression -->
            <h2 class="fade-in">üìà What is Linear Regression?</h2>
            <p class="fade-in">
              Linear regression is a fundamental statistical method used to model the relationship between a dependent variable and one or more independent variables. It assumes a linear relationship, meaning the outcome can be predicted by fitting a straight line through the data points.
              The model is represented by the equation: <code>y = Œ≤‚ÇÄ + Œ≤‚ÇÅx + Œµ</code>, where <code>y</code> is the predicted value, <code>x</code> is the feature, and <code>Œµ</code> is the error term.
            </p>
            <div class="gif-section">
                <img src="assets/linear_regression_line.gif" alt="linear regression" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>
            <p class="fade-in">
              It's primarily used in regression problems‚Äîsuch as predicting calorie content from fat, protein, or sugar levels. In food science, this helps forecast nutrient totals or cost based on ingredients.
            </p>
      
            <!-- (b) Logistic Regression -->
            <h2 class="fade-in">üìâ What is Logistic Regression?</h2>
            <p class="fade-in">
              Logistic regression is used for classification problems. It predicts the probability of a sample belonging to a particular class using a logistic (sigmoid) function to transform the linear combination of inputs into a probability between 0 and 1.
            </p>
            <div class="gif-section">
                <img src="assets/logistic_regression_curve.gif" alt="logistic regression" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>
            <p class="fade-in">
              The model is defined by: <code>P(y=1) = 1 / (1 + e^-(Œ≤‚ÇÄ + Œ≤‚ÇÅx))</code>. It‚Äôs widely used in food analysis to classify items as healthy/unhealthy or vegan/non-vegan based on nutritional features.
            </p>
      
            <!-- (c) Similarities and Differences -->
            <h2 class="fade-in">üîç Similarities and Differences</h2>
            <p class="fade-in">
              Both linear and logistic regression use linear equations to model relationships between variables, and both estimate coefficients using optimization techniques.
            </p>
            <div class="image-container">
              <img src="assets/linear_vs_logistic.png" alt="Linear vs Logistic Regression" class="topic-image">
            </div>
            <p class="fade-in">
              However, linear regression predicts continuous outcomes, while logistic regression predicts probabilities for categorical outcomes. Logistic regression maps its output using the sigmoid function, whereas linear regression outputs raw values.
            </p>
      
            <!-- (d) Sigmoid Function -->
            <h2 class="fade-in">üßÆ Does Logistic Regression Use the Sigmoid Function?</h2>
            <p class="fade-in">
              Yes, logistic regression uses the sigmoid function to map any real-valued number into a value between 0 and 1. This output is interpreted as the probability of the positive class.
            </p>
            <div class="image-container">
              <img src="assets/sigmoid_function.png" alt="Sigmoid Function Plot" class="topic-image">
            </div>
            <p class="fade-in">
              The sigmoid function is defined as: <code>S(x) = 1 / (1 + e^(-x))</code>. This makes it ideal for binary classification where the goal is to predict whether a food belongs to a category (e.g., "Contains Gluten": Yes/No).
            </p>
      
            <!-- (e) Maximum Likelihood -->
            <h2 class="fade-in">üìä How is Maximum Likelihood Connected to Logistic Regression?</h2>
            <p class="fade-in">
              Logistic regression uses **Maximum Likelihood Estimation (MLE)** to find the parameters (weights) that maximize the probability of correctly classifying the training data.
            </p>
            <div class="image-container">
              <img src="assets/log_likelihood.png" alt="Log-Likelihood Maximization" class="topic-image">
            </div>
            <p class="fade-in">
              Instead of minimizing squared error (like linear regression), it maximizes the log-likelihood‚Äîa measure of how likely the observed labels are, given the model predictions. This optimization enables robust probability-based classification.
            </p>

            <!-- (a) Dataset Overview -->
            <h2>Dataset Overview</h2>
            <p class="fade-in">
                The dataset used for this binary classification task is <code>clean_normalized_data.csv</code>, consisting of nutritional information for thousands of food items. For this analysis, we filtered the dataset to retain only two categories: <strong>Candy</strong> and <strong>Cookies &amp; Biscuits</strong>. These were labeled as 0 and 1 respectively, forming the basis for binary classification.
            </p>
            <div class="image-container">
                <img src="assets/clean_normalized_data.png" alt="Clean Normalized Data Preview" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/clean_normalized_data.csv" target="_blank" class="github-button">View Dataset</a>
            </div>

            <!-- (b) Data Preparation -->
            <h2>Data Preparation</h2>
            <p class="fade-in">
                Features and labels were extracted after filtering the dataset. Three columns ‚Äî <code>description</code>, <code>category</code>, and <code>brand</code> ‚Äî were dropped to retain only numeric features. The dataset was then split into training and testing sets using an 80:20 stratified split to maintain class balance.
            </p>

            <h4 class="fade-in">üìÅ X_test: Feature matrix for evaluation</h4>
            <div class="image-container">
                <img src="assets/logreg_X_test.png" alt="logreg_X_test Preview" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/logreg_X_test.csv" target="_blank" class="github-button">View logreg_X_test.csv</a>
            </div>

            <h4 class="fade-in">üìÅ X_train: Feature matrix for training</h4>
            <div class="image-container">
                <img src="assets/logreg_X_train.png" alt="logreg_X_train Preview" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/logreg_X_train.csv" target="_blank" class="github-button">View logreg_X_train.csv</a>
            </div>

            <h4 class="fade-in">üè∑Ô∏è y_test: Ground truth labels for evaluation</h4>
            <div class="image-container">
                <img src="assets/logreg_Y_test.png" alt="logreg_Y_test Preview" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/logreg_y_test.csv" target="_blank" class="github-button">View logreg_y_test.csv</a>
            </div>

            <h4 class="fade-in">üè∑Ô∏è y_train: Labels for training</h4>
            <div class="image-container">
                <img src="assets/logreg_Y_train.png" alt="logreg_Y_train Preview" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/logreg_y_train.csv" target="_blank" class="github-button">View logreg_y_train.csv</a>
            </div>

            <!-- (c) Code -->
            <h2> Modeling Code</h2>

            <p class="fade-in">
                The modeling script brings together the full classification workflow using <strong>Scikit-learn</strong> ‚Äî one of the most widely used Python libraries for machine learning. We implemented and compared two classic supervised learning algorithms: <strong>Logistic Regression</strong> and <strong>Multinomial Naive Bayes</strong>, specifically tailored for a binary classification task between <code>Candy</code> and <code>Cookies & Biscuits</code>.
            </p>

            <p class="fade-in">
                The dataset was first split into training and testing subsets using an <code>80:20 stratified split</code> to preserve class balance across both sets. The features were already normalized, but since <strong>Multinomial Naive Bayes (MNB)</strong> requires all input features to be non-negative, we applied a <strong>feature-wise shift</strong> (i.e., subtracting the minimum of each feature from all rows) before feeding it into MNB. <strong>Logistic Regression</strong> did not require any such adjustment and was trained directly on the original normalized data.
            </p>

            <p class="fade-in">
                After training both models on the processed data:
                <ul class="fade-in">
                    <li>We predicted labels for the test set using each model</li>
                    <li>Generated <strong>confusion matrices</strong> to visualize prediction vs actual performance</li>
                    <li>Created <strong>classification reports</strong> summarizing precision, recall, F1-score, and support</li>
                    <li>Saved visualizations as PNGs and reports as TXT files for documentation</li>
                </ul>
            </p>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/logreg_vs_mnb_compare.py" target="_blank" class="github-button">üîç View Modeling Script</a>
            </div>

            <!-- (d) Results & Visualizations -->
            <h2>Results & Visualizations</h2>

            <h3 class="fade-in">Logistic Regression</h3>
            <p class="fade-in">
                The Logistic Regression model was trained on normalized nutrient values and performed well in classifying food items as either <strong>Candy (0)</strong> or <strong>Cookies & Biscuits (1)</strong>. The confusion matrix below shows that the model correctly predicted 303 out of 392 Candy items and 467 out of 479 Cookie items.
            </p>
            <p class="fade-in">
                However, the model misclassified 89 Candy items as Cookies and only 12 Cookies as Candy, indicating a slight imbalance in precision for class 0. This suggests the model has a stronger tendency to predict Cookies over Candy, possibly due to overlapping nutrient profiles.
            </p>

            <!-- Logistic Confusion Matrix -->
            <div class="image-container">
                <img src="visuals/logreg_confusion.png" alt="Logistic Regression Confusion Matrix" class="topic-image">
            </div>

            <!-- Logistic Report Image -->
            <div class="image-container">
                <img src="assets/logreg_report.png" alt="Logistic Regression Report" class="topic-image">
            </div>

            <!-- Logistic Report Button -->
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/logreg_report.txt" target="_blank" class="github-button">View Logistic Report</a>
            </div>

            <h3 class="fade-in">Multinomial Naive Bayes</h3>
            <p class="fade-in">
                The Multinomial Naive Bayes (MNB) model was trained on the same features after applying a shift to ensure all values were positive (as required by MNB). The confusion matrix shows that MNB slightly outperforms Logistic Regression ‚Äî correctly classifying 304 Candy items and 467 Cookie items, with only 12 misclassified Cookies and 88 misclassified Candies.
            </p>
            <p class="fade-in">
                The classification report reveals that the model maintains strong precision and recall across both classes, achieving an overall accuracy of <strong>88.52%</strong> compared to Logistic Regression‚Äôs <strong>88.40%</strong>. The improvement, although marginal, highlights MNB‚Äôs effectiveness on this type of categorical data with normalized counts.
            </p>

            <!-- MNB Confusion Matrix -->
            <div class="image-container">
                <img src="visuals/mnb_confusion.png" alt="MNB Confusion Matrix" class="topic-image">
            </div>

            <!-- MNB Report Image -->
            <div class="image-container">
                <img src="assets/mnb_report.png" alt="MNB Report" class="topic-image">
            </div>

            <!-- MNB Report Button -->
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/mnb_report.txt" target="_blank" class="github-button">View MNB Report</a>
            </div>

            <!-- (e) Model Comparison & Interpretation -->
            <h2>üìä Model Comparison & Interpretation</h2>

            <p class="fade-in">
                After training both <strong>Logistic Regression</strong> and <strong>Multinomial Naive Bayes</strong> (MNB) models on the same cleaned and normalized dataset, we evaluated their performance using confusion matrices, classification reports, and accuracy metrics. Both models demonstrated strong classification capabilities for distinguishing between <strong>Candy</strong> and <strong>Cookies & Biscuits</strong> based on nutrient profiles.
            </p>

            <p class="fade-in">
                The <strong>Multinomial Naive Bayes</strong> model achieved an overall accuracy of <code>88.52%</code>, slightly outperforming <strong>Logistic Regression</strong>, which reached <code>88.40%</code>. This difference, while subtle, reflects how MNB benefits from its probabilistic structure ‚Äî especially when working with nutrient data that resembles frequency or scaled count distributions. Since MNB assumes feature independence and handles skewed distributions well, it's particularly effective in nutrition-related datasets where certain macro-nutrients (e.g., carbs or fats) dominate.
            </p>

            <p class="fade-in">
                In both models, <strong>recall for class 1 (Cookies & Biscuits)</strong> was excellent (0.97), indicating the models almost always correctly identify Cookie items. However, the <strong>recall for class 0 (Candy)</strong> was notably lower, especially in Logistic Regression (0.7730) compared to MNB (0.7755). This suggests that <strong>Candy items were more likely to be misclassified</strong>, likely due to feature overlaps with other sugary items like certain biscuits or chocolate-covered snacks.
            </p>

            <p class="fade-in">
                From the confusion matrices:
                <ul class="fade-in">
                    <li>Logistic Regression misclassified 89 Candy items as Cookies</li>
                    <li>Multinomial Naive Bayes misclassified 88 Candy items</li>
                    <li>Both models misclassified only 12 Cookie items</li>
                </ul>
                These numbers highlight that <strong>class 0 (Candy)</strong> was harder to identify correctly. This may be due to more variance in nutrient values within Candy, or an imbalance in distribution even after stratified splitting.
            </p>

            <p class="fade-in">
                Overall, while <strong>Logistic Regression</strong> is known for its interpretability and stability, especially in linearly separable problems, <strong>MNB</strong> proves to be more effective here because:
                <ul class="fade-in">
                    <li>It handles high-dimensional data better</li>
                    <li>Performs well with positively scaled normalized values</li>
                    <li>Is less sensitive to multicollinearity and more efficient in memory</li>
                </ul>
            </p>

            <p class="fade-in">
                Both models have strengths, but in this specific case of binary food classification using nutrient values, <strong>Multinomial Naive Bayes offers a performance edge</strong> while maintaining simplicity and speed.
            </p>

            <!-- (f) Conclusion -->
            <h2>üß† Final Conclusion & Takeaways</h2>
            <p class="fade-in">
                This section summarizes the learnings and outcomes of this binary classification task using real-world nutritional data sourced from the USDA database. Here are the key highlights:
            </p>

            <ul class="fade-in">
                <li>
                    ‚úÖ The dataset was successfully filtered to contain only two relevant and nutritionally distinct food categories: <strong>Candy</strong> and <strong>Cookies & Biscuits</strong>, allowing for a well-defined binary classification problem.
                </li>
                <li>
                    üîÅ Feature preparation involved dropping non-numeric identifiers like <code>description</code>, <code>brand</code>, and <code>category</code>, followed by normalization to ensure model compatibility and fair scaling.
                </li>
                <li>
                    üß™ Both <strong>Logistic Regression</strong> and <strong>Multinomial Naive Bayes</strong> were implemented using <code>Scikit-learn</code>. Confusion matrices and classification reports provided deep insights into precision, recall, and model confidence.
                </li>
                <li>
                    üìà <strong>MNB showed a slight accuracy advantage</strong> and better performance in handling feature variability, making it the recommended model for future deployment in similar nutrition-centric classification tasks.
                </li>
                <li>
                    üîó All visuals (confusion matrices, performance reports) and raw CSV data splits were preserved and linked through GitHub, ensuring full transparency and reproducibility of the modeling process.
                </li>
            </ul>

            <p class="fade-in">
                In conclusion, this task exemplifies a full-cycle machine learning pipeline: from dataset curation and transformation to model selection, training, and evaluation. The project highlights the real-world value of classic models like Logistic Regression and Naive Bayes ‚Äî and how they can be leveraged effectively for health and nutrition analytics.
            </p>

            <p class="fade-in">
                With minimal tuning, interpretable features, and a strong understanding of model assumptions, such approaches can enable smarter food classification, better consumer health recommendations, and scalable decision-making systems in dietary tech.
            </p>

    </div>

        <!-- Your existing HTML content -->

        <script>
            document.addEventListener("DOMContentLoaded", function() {
                let text = "Regression";
                let i = 0;
                function typeWriter() {
                    if (i < text.length) {
                        document.getElementById("typing-title").innerHTML += text.charAt(i);
                        i++;
                        setTimeout(typeWriter, 50);
                    }
                }
                typeWriter();
            });
        </script>

    <!-- üîπ JavaScript for Scroll-Based Fade-In Effect -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            let fadeInElements = document.querySelectorAll(".fade-in");

            function fadeInOnScroll() {
                fadeInElements.forEach(element => {
                    let position = element.getBoundingClientRect().top;
                    let screenHeight = window.innerHeight;

                    if (position < screenHeight - 100) {
                        element.classList.add("visible");
                    }
                });
            }

            window.addEventListener("scroll", fadeInOnScroll);
            fadeInOnScroll(); // Trigger on load
        });
    </script>

    <script>
        function toggleMenu() {
            var navbar = document.querySelector(".navbar");
            navbar.classList.toggle("active");
        }
    </script>
    <script type="module" src="animations.js"></script>
    <div class="transition-overlay"></div>
    <script type="module" src="animations.js"></script>
</body>
</html>