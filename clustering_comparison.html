<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Clustering Method</th>
      <th>Concept</th>
      <th>Advantages</th>
      <th>Limitations</th>
      <th>Best Use Cases</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>K-Means</td>
      <td>Partitions data into k clusters by minimizing variance within clusters.</td>
      <td>âœ”ï¸ Efficient for large datasets.\nâœ”ï¸ Works well for well-separated clusters.\nâœ”ï¸ Computationally fast.</td>
      <td>âŒ Assumes spherical clusters.\nâŒ Sensitive to outliers.\nâŒ Requires specifying k.</td>
      <td>ğŸ“Œ When clusters are compact and well-separated.\nğŸ“Œ When a fast and scalable solution is needed.</td>
    </tr>
    <tr>
      <td>Hierarchical Clustering</td>
      <td>Builds a nested hierarchy of clusters by merging or splitting groups based on distance.</td>
      <td>âœ”ï¸ No need to specify k.\nâœ”ï¸ Produces dendrograms for visualization.\nâœ”ï¸ Captures hierarchical relationships.</td>
      <td>âŒ Computationally expensive for large datasets.\nâŒ Can be sensitive to minor data variations.\nâŒ Difficult to scale.</td>
      <td>ğŸ“Œ When understanding relationships between clusters is important.\nğŸ“Œ Works well for small datasets.</td>
    </tr>
    <tr>
      <td>DBSCAN</td>
      <td>Groups points based on density, marking outliers as noise.</td>
      <td>âœ”ï¸ Automatically determines clusters.\nâœ”ï¸ Detects arbitrary-shaped clusters.\nâœ”ï¸ Identifies outliers.</td>
      <td>âŒ Requires careful tuning of epsilon &amp; minPts.\nâŒ Struggles with varying densities.\nâŒ May classify dense noise as clusters.</td>
      <td>ğŸ“Œ When outlier detection is needed.\nğŸ“Œ When clusters have irregular shapes.\nğŸ“Œ When the number of clusters is unknown.</td>
    </tr>
  </tbody>
</table>