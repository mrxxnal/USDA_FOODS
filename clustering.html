<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Clustering</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>

    <!-- Elegant Animated Background -->
    <div class="food-background"></div>
    <div class="light-overlay"></div>

    <!-- Navigation Bar -->
    <div class="menu-icon" onclick="toggleMenu()">☰</div>
    <div class="navbar">
        <a href="index.html">Home</a>
        <a href="introduction.html">Introduction</a>
        <a href="data_prep.html">Data Prep</a>
        <a href="eda.html">EDA</a>
        <a href="pca.html">PCA</a>
        <a href="clustering.html" class="active">Clustering</a>
        <a href="arm.html">ARM</a>
        <a href="decision_trees.html">Decision Trees</a>
        <a href="naive_bayes.html">Naive Bayes</a>
        <a href="svm.html">SVM</a>
        <a href="regression.html">Regression</a>
        <a href="randomforest.html">Random Forest</a>
        <a href="conclusions.html">Conclusions</a>
        <a href="about_me.html">About Me</a>
    </div>

    <div class="content-section">
        <h1 id="typing-title"></h1>
    
        <div class="content-wrapper">
            <!-- Introduction to Clustering -->
            <h2>Introduction to Clustering</h2>

            <p class="fade-in">
                <strong>Clustering</strong> is a cornerstone technique in unsupervised machine learning, where the goal is to automatically discover natural groupings within data without any predefined labels or categories. 
                Unlike supervised learning—which relies on known outcomes—clustering invites the algorithm to find its own structure within the dataset, based purely on similarities among data points. 
                This approach is widely used in real-world applications such as <strong>customer segmentation</strong> for personalized marketing, <strong>anomaly detection</strong> in cybersecurity, and <strong>biological data analysis</strong> to identify gene or protein families.
                In the context of this project, clustering offers a powerful, unbiased way to explore and categorize food items based entirely on their <strong>nutritional profiles</strong>, without prior assumptions about how foods should be grouped. 
                By analyzing attributes like calories, sugars, fats, fiber, and protein content, clustering enables the discovery of hidden dietary patterns that may not be immediately obvious through traditional classification. 
                Ultimately, applying clustering to nutrition data helps reveal underlying trends in food composition, consumer preferences, and even potential nutritional gaps—laying the foundation for smarter, more personalized food recommendations.
            </p>

            <div class="gif-section">
                <img src="assets/kmeans.gif" alt="K-Means Clustering Animation" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>
    
            <p class="fade-in">
                In this project, three distinct clustering techniques were explored, each offering a unique lens through which to understand the structure of nutritional data:
            </p>
            
            <ul class="fade-in">
                <li><strong>K-Means Clustering:</strong> A popular partition-based method that divides data points into a predefined number of clusters by minimizing the distance between points and their assigned cluster centers. It excels when the data exhibits clear, spherical groupings and provides an efficient, scalable approach to clustering large datasets.</li>
                <li><strong>Hierarchical Clustering:</strong> An agglomerative technique that builds a tree-like structure known as a <em>dendrogram</em>, revealing nested relationships between food items. It does not require predefining the number of clusters, making it ideal for uncovering multi-level hierarchies and understanding how closely different items are related at various levels of granularity.</li>
                <li><strong>DBSCAN (Density-Based Spatial Clustering of Applications with Noise):</strong> A powerful density-based algorithm that identifies clusters based on areas of high data point density, while simultaneously detecting outliers. Unlike K-Means, DBSCAN can find clusters of arbitrary shapes and is exceptionally robust to noise, making it ideal for complex, non-linear nutritional patterns where traditional partitioning methods might fail.</li>
            </ul>
    
            <div class="image-container">
                <img src="assets/comparison_clusters.png" alt="Comparison of Clustering Methods" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>
    
            <p class="fade-in">
                Each clustering method brings its own strengths and limitations to the table, offering diverse insights into the underlying nutritional patterns:
            </p>
            
            <ul class="fade-in">
                <li><strong>K-Means Clustering:</strong> Highly efficient and scalable, making it ideal for large datasets. However, it operates best when clusters are spherical and similarly sized, which may not always align with the natural structure of real-world nutritional data.</li>
                <li><strong>Hierarchical Clustering:</strong> Provides a detailed, tree-like view of data relationships, capturing multiple layers of similarity between food items. While it excels at revealing complex hierarchies, it can be computationally intensive, particularly for larger datasets.</li>
                <li><strong>DBSCAN:</strong> Exceptionally good at detecting clusters of arbitrary shapes and identifying outliers, making it a powerful tool for irregular data landscapes. However, it requires careful tuning of parameters like epsilon and minimum points, which can influence the quality and stability of the resulting clusters.</li>
            </ul>

            <!-- Distance Metrics in Clustering -->
            <h2>Understanding Distance Metrics in Clustering</h2>
            <p class="fade-in">
                Clustering algorithms rely on <strong>distance metrics</strong> to measure how similar or different data points are. 
                The choice of metric impacts cluster formation and overall accuracy. Below are common distance measures used in <strong>K-Means</strong>, <strong>Hierarchical Clustering</strong>, and <strong>DBSCAN</strong>.
            </p>

            <!-- Image Representation -->
            <div class="image-container">
                <img src="assets/distance_metrics.png" alt="Comparison of Distance Metrics" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>

            <h3>Common Distance Metrics</h3>

            <ul class="fade-in">
                <li><strong>Euclidean Distance:</strong> Often visualized as the straight-line distance between two points in space, Euclidean distance is the most intuitive and widely used metric. In clustering algorithms like <strong>K-Means</strong> and <strong>DBSCAN</strong>, it helps in grouping points that are closer together geometrically. It assumes the data lies in a continuous, isotropic space where proximity equates to similarity. However, Euclidean distance may become less effective in very high-dimensional datasets, where the "curse of dimensionality" can distort true similarities.</li>

                <li><strong>Cosine Similarity:</strong> Instead of focusing on distance, cosine similarity measures the orientation between two vectors. It calculates the cosine of the angle separating them, making it ideal for comparing text documents or high-dimensional datasets where magnitude differences are less important than directional trends. Used heavily in <strong>text clustering</strong> and often in <strong>hierarchical clustering</strong>, this metric shines when the actual values matter less than the overall shape or composition of the vectors.</li>

                <li><strong>Hamming Distance:</strong> Specifically tailored for categorical or binary data, Hamming distance counts the number of positions at which corresponding elements differ. It is particularly useful when clustering data like genetic sequences, text encoded as binary, or survey responses. Hamming distance provides a simple yet powerful way to quantify dissimilarity when attributes are not numeric but rather binary outcomes like "yes/no" or "present/absent."</li>

                <li><strong>Manhattan Distance:</strong> Also known as "taxicab" distance, Manhattan distance measures the sum of absolute differences across dimensions. This metric mimics how a taxi would navigate a city grid—turning at intersections rather than traveling diagonally. It works well for <strong>K-Means</strong> clustering when feature scales differ or when the data structure inherently favors stepwise (grid-like) movement rather than straight-line measurements.</li>

                <li><strong>Minkowski Distance:</strong> A generalization that encompasses both Euclidean and Manhattan distances. By adjusting the parameter <strong>p</strong>, Minkowski distance can be tuned to behave like Manhattan (<strong>p=1</strong>) or Euclidean (<strong>p=2</strong>). It provides a flexible tool for clustering, allowing researchers to experiment and choose a distance metric that best matches their data’s underlying geometry.</li>

                <li><strong>Chebyshev Distance:</strong> Instead of summing or squaring differences, Chebyshev distance simply finds the largest absolute difference along any coordinate dimension. It's particularly useful in environments where one dominant feature can determine cluster membership—such as maximum deviation in a critical nutrient or ingredient that sharply separates different types of food products.</li>

                <li><strong>Jaccard Similarity:</strong> A set-based metric that measures the proportion of shared elements between two sets. Jaccard is excellent for clustering binary, categorical, or text data where overlap matters more than absolute counts. It’s commonly used in applications like document clustering, social network analysis, and biological data comparisons where "intersection over union" provides meaningful similarity estimates.</li>

                <li><strong>Haversine Distance:</strong> Specially designed for measuring the shortest distance between two points on a sphere, Haversine distance is indispensable in <strong>geo-spatial clustering</strong>—such as mapping food consumption patterns across different regions or studying regional dietary variations. It accounts for the Earth's curvature, making it ideal for clustering based on real-world geographic coordinates.</li>

                <li><strong>Sørensen-Dice Coefficient:</strong> Similar in spirit to Jaccard similarity but more sensitive to common elements. The Sørensen-Dice coefficient doubles the weight of shared features, making it especially suited for biological clustering tasks, string matching, and medical data analysis where even slight overlaps carry important meaning.</li>
            </ul>

            <h3>Impact of Distance Metrics on Clustering</h3>

            <p class="fade-in">
                Different clustering algorithms are tailored to work best with particular distance metrics, and the choice of metric can significantly impact the quality and interpretability of clusters:
            </p>

            <ul class="fade-in">
                <li><strong>K-Means:</strong> Primarily utilizes <strong>Euclidean distance</strong> to create spherical clusters. As a result, it assumes that all clusters have similar density and size, which can sometimes limit its ability to accurately group irregularly distributed food items or highly skewed nutrient profiles.</li>

                <li><strong>Hierarchical Clustering:</strong> Is flexible with distance metrics, allowing practitioners to select <strong>cosine similarity</strong>, <strong>Jaccard distance</strong>, or <strong>Euclidean distance</strong> depending on the nature of the data. This adaptability enables hierarchical models to uncover rich multi-level structures in both numerical and categorical datasets, like layered similarities between processed and natural foods.</li>

                <li><strong>DBSCAN:</strong> Typically uses <strong>Euclidean distance</strong> but is not restricted to it. It can be modified to incorporate custom distance metrics like Haversine for spatial data or Manhattan for grid-like distributions. Because DBSCAN relies on density estimation rather than shape assumptions, choosing the right distance metric is crucial to accurately detect organic cluster boundaries and outliers.</li>
            </ul>

            <p class="fade-in">
                Choosing the right distance metric is crucial for obtaining meaningful clusters. For datasets with high-dimensional features, <strong>cosine similarity</strong> is often more effective, while for continuous numerical data, <strong>Euclidean distance</strong> remains a standard choice.
            </p>
    
            <h2>Clustering Implementation: Dataset Overview</h2>
            <p class="fade-in">
                The dataset used for clustering consists of various <strong>food items</strong> with multiple <strong>nutritional attributes</strong>, such as macronutrient composition, vitamin content, and caloric density. 
                The goal is to identify natural groupings within the dataset that reflect dietary patterns and food classifications.
            </p>
    
            <h3>Feature Selection</h3>
            <p class="fade-in">
                To ensure the clustering algorithms work effectively, only numerical attributes were retained. The following transformations were applied:
            </p>
    
            <ul class="fade-in">
                <li><strong>Removal of categorical attributes:</strong> Non-numeric data such as food brand names were excluded.</li>
                <li><strong>Selection of key nutritional metrics:</strong> Macronutrient composition (carbohydrates, proteins, fats) and energy content were prioritized.</li>
            </ul>
    
            <!-- Data Preparation -->
            <h2>Data Preparation</h2>
            <p class="fade-in">
                Before applying clustering algorithms, the dataset underwent rigorous preprocessing. This step is critical to ensure that clustering methods operate efficiently and accurately. The preprocessing steps included:
            </p>
    
            <ul class="fade-in">
                <li><strong>Handling Missing Values:</strong> Missing numerical values were imputed using the median or mean, ensuring consistency.</li>
                <li><strong>Feature Scaling:</strong> StandardScaler was applied to normalize features, preventing attributes with larger ranges from dominating the clustering process.</li>
                <li><strong>Dimensionality Reduction:</strong> Principal Component Analysis (PCA) was used to reduce redundant information and optimize computation.</li>
            </ul>
    
            <div class="image-container">
                <img src="assets/clean_normalized_data.png" alt="Clean and Normalized Data Preview" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>
    
            <p class="fade-in">
                The visualization above shows the dataset after normalization. Scaling ensures that all features contribute equally to distance calculations used in clustering.
            </p>
    
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/clean_normalized_data.csv" target="_blank" class="github-button">
                    View Clean Normalized Data
                </a>
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/clustering_dataprep.py" target="_blank" class="github-button">
                    View Clustering Data Preparation Script
                </a>
            </div>
    
            <!-- Generated Clustering Data -->
            <h2>Generated Clustering Data</h2>
            <p class="fade-in">
                After preprocessing, the dataset was structured in a format suitable for clustering. The processed dataset contains only <strong>numerical values</strong> for clustering algorithms to analyze effectively.
            </p>
    
            <p class="fade-in">
                The table below provides a preview of the cleaned dataset, ready for clustering:
            </p>
    
            <div class="image-container">
                <img src="assets/clustering_data.png" alt="Clustering Data Preview" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>
    
            <p class="fade-in">
                Key transformations applied to this dataset:
            </p>
    
            <ul class="fade-in">
                <li><strong>Feature Scaling:</strong> Standardized all numerical attributes to have zero mean and unit variance.</li>
                <li><strong>Dimensionality Reduction:</strong> Reduced the dataset to key components that explain most of the variance.</li>
                <li><strong>Cluster Compatibility:</strong> Removed outliers and ensured that all remaining features were suitable for clustering analysis.</li>
            </ul>
    
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/clustering_data.csv" target="_blank" class="github-button">
                    View Clustering Data
                </a>
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/original_labels.csv" target="_blank" class="github-button">
                    View Original Labels
                </a>
            </div>
    
            <h2>Summary of Data Preparation</h2>
            <p class="fade-in">
                The data preprocessing steps were crucial in ensuring high-quality input for clustering. The key takeaways from this process are:
            </p>
    
            <ul class="fade-in">
                <li>All <strong>categorical variables</strong> were removed to ensure clustering algorithms work optimally.</li>
                <li><strong>Missing values</strong> were handled efficiently, preventing any data bias.</li>
                <li><strong>Feature scaling</strong> ensured that numerical attributes contributed equally to clustering.</li>
                <li><strong>PCA transformation</strong> helped reduce dimensionality while retaining the most important information.</li>
            </ul>
    
            <p class="fade-in">
                With the dataset now optimized for clustering, we proceed to apply the K-Means, Hierarchical, and DBSCAN clustering methods to uncover patterns in food classification.
            </p>

            <!-- K-Means Clustering -->
            <h2>K-Means Clustering</h2>

            <div class="gif-section">
                <img src="assets/k_means_clustering.gif" alt="K-Means Clustering Animation" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>

            <p class="fade-in">
                <strong>K-Means</strong> is a widely used clustering algorithm that partitions data into a predefined number of clusters (<strong>k</strong>).
                It operates iteratively to assign data points to the nearest cluster centroid while updating the centroids until convergence. 
                The objective is to minimize intra-cluster variance, ensuring that data points within the same cluster are as similar as possible.
            </p>

            <h3>Determining the Optimal Number of Clusters</h3>
            <p class="fade-in">
                Selecting the appropriate number of clusters (<strong>k</strong>) is crucial for achieving meaningful clustering. 
                A poorly chosen k-value can lead to underfitting (too few clusters) or overfitting (too many clusters). 
                To determine the optimal number of clusters, we used the <strong>Silhouette Score</strong>, 
                which measures the compactness of clusters and their separation from other clusters. A higher silhouette score indicates well-defined clusters.
            </p>

            <!-- Silhouette Score Graph -->
            <div class="image-container">
                <img src="visuals/KMeans_Silhouette_Scores.png" alt="Silhouette Scores for K-Means" class="topic-image">
            </div>

            <p class="fade-in">
                The plot above illustrates how the silhouette score varies across different values of k. The peaks in the graph represent the most suitable choices for k, 
                as they indicate the best balance between cluster compactness and separation. Based on this analysis, the three most effective k-values were selected for clustering.
            </p>

            <h3>3D K-Means Clustering Visualization</h3>
            <p class="fade-in">
                After determining the optimal k-values, <strong>K-Means clustering</strong> was applied to the dataset. 
                To better visualize how the clusters are formed, we used <strong>Principal Component Analysis (PCA)</strong> to reduce the dataset to three principal components (PC1, PC2, PC3), 
                which allowed for a clear 3D representation of the clustering results.
            </p>

            <!-- 3D K-Means Clustering Visualization -->
            <div class="image-container">
                <img src="visuals/KMeans_Clusters_3D.png" alt="3D K-Means Clustering" class="topic-image">
            </div>

            <p class="fade-in">
                The visualization above represents K-Means clustering for three different values of k. Each color represents a distinct cluster, and the red markers indicate cluster centroids. 
                The plots highlight how increasing k influences cluster separation. Some key observations:
            </p>

            <ul class="fade-in">
                <li>For <strong>k=2</strong>, the algorithm splits the data into two large groups, which may be too broad to capture nuanced structures.</li>
                <li>For <strong>k=6</strong>, the clusters become more refined, but some smaller clusters may not have strong differentiation.</li>
                <li>For <strong>k=5</strong>, the balance between compactness and meaningful separation appears optimal.</li>
            </ul>

            <h3>Why These K Values?</h3>
            <p class="fade-in">
                The <strong>Silhouette Score analysis</strong> was instrumental in determining the most suitable k-values. 
                The highest peaks in the silhouette score plot indicate the values of k where clusters are <strong>most well-defined</strong> and <strong>least overlapping</strong>.
            </p>

            <ul class="fade-in">
                <li><strong>k = 2</strong>: Simpler division but lacks specificity in distinguishing different groups.</li>
                <li><strong>k = 5</strong>: Strikes a balance between cluster cohesion and interpretability.</li>
                <li><strong>k = 6</strong>: Offers more refined groupings but may introduce unnecessary complexity.</li>
            </ul>

            <p class="fade-in">
                These selected k-values provided the most <strong>stable and interpretable clusters</strong> while avoiding over-segmentation of the data.
            </p>

            <h3>Observations</h3>
            <p class="fade-in">
                The application of K-Means clustering to the dataset led to the following observations:
            </p>

            <ul class="fade-in">
                <li><strong>K-Means effectively identified meaningful patterns</strong> in the dataset, separating different food categories based on nutritional composition.</li>
                <li>The <strong>Silhouette Score guided the selection of optimal k-values</strong>, ensuring compact and well-separated clusters.</li>
                <li>Some clusters exhibit slight overlaps, indicating that alternative clustering methods like <strong>DBSCAN</strong> might be more effective in identifying varying densities.</li>
            </ul>

            <!-- Hierarchical Clustering -->
                <h2>Hierarchical Clustering</h2>

                <div class="gif-section">
                    <img src="assets/hierarchial.gif" alt="K-Means Clustering Animation" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
                </div>

                <p class="fade-in">
                    <strong>Hierarchical Clustering</strong> is a bottom-up clustering technique that builds a hierarchy of clusters by progressively merging smaller clusters into larger ones. 
                    Unlike K-Means, it does <strong>not require a predefined number of clusters</strong>. Instead, it generates a tree-like structure called a <strong>dendrogram</strong>, which 
                    visually represents how data points are grouped at various levels of similarity.
                </p>

                <!-- Dendrogram -->
                <h3>Dendrogram Analysis</h3>
                <p class="fade-in">
                    A <strong>dendrogram</strong> is a visual representation of the clustering hierarchy. Each point in the tree represents a cluster, and the height at which two clusters merge 
                    indicates their similarity. The higher the merge, the greater the difference between clusters.
                </p>

                <div class="image-container">
                    <img src="visuals/hierarchical_dendrogram.png" alt="Hierarchical Clustering Dendrogram" class="topic-image">
                </div>

                <p class="fade-in">
                    The <strong>above dendrogram</strong> helps in determining the optimal number of clusters. By cutting the dendrogram at different heights, we can analyze different levels 
                    of cluster granularity. This flexibility allows us to explore relationships between food groups without a strict assumption about the number of clusters.
                </p>

                <!-- 3D Hierarchical Clustering -->
                <h3>3D Visualization of Hierarchical Clustering</h3>
                <p class="fade-in">
                    To better understand how hierarchical clustering grouped our dataset, we visualized the clusters using a <strong>3D PCA transformation</strong>. This allows us to observe 
                    how food items are distributed based on their nutrient compositions.
                </p>

                <div class="image-container">
                    <img src="visuals/hierarchical_clustering.png" alt="3D Hierarchical Clustering" class="topic-image">
                </div>

                <p class="fade-in">
                    In the visualization above, different colors represent distinct clusters identified by hierarchical clustering. Unlike K-Means, which forces each data point into a cluster, 
                    <strong>hierarchical clustering preserves the underlying structure</strong> and allows us to observe relationships between different groups at varying levels of similarity.
                </p>

                <!-- Key Advantages -->
                <h3>Why Choose Hierarchical Clustering?</h3>
                <p class="fade-in">
                    Hierarchical Clustering offers several advantages compared to traditional clustering methods like K-Means:
                </p>

                <ul class="fade-in">
                    <li><strong>No need to predefine the number of clusters:</strong> Unlike K-Means, which requires choosing k in advance, hierarchical clustering allows us to explore the dataset naturally.</li>
                    <li><strong>Provides a complete hierarchy:</strong> The dendrogram gives a detailed visualization of how clusters are formed at different levels.</li>
                    <li><strong>Works well for small to medium-sized datasets:</strong> Since it calculates all pairwise distances, it is computationally expensive for very large datasets but excellent for detailed analysis.</li>
                    <li><strong>Identifies nested structures:</strong> It is useful when clusters have sub-clusters within them, allowing a more refined categorization.</li>
                </ul>

                <!-- Limitations -->
                <h3>Limitations of Hierarchical Clustering</h3>
                <p class="fade-in">
                    Despite its strengths, hierarchical clustering has some limitations:
                </p>

                <ul class="fade-in">
                    <li><strong>Computationally expensive:</strong> The algorithm has a complexity of O(n²), making it inefficient for very large datasets.</li>
                    <li><strong>Sensitive to noise and outliers:</strong> Since it considers all data points in a distance matrix, outliers can distort the clustering structure.</li>
                    <li><strong>Inflexibility after clustering:</strong> Once the hierarchy is built, it cannot be modified without recalculating the entire dendrogram.</li>
                </ul>

                <!-- Observations -->
                <h3>Observations and Key Insights</h3>
                <p class="fade-in">
                    - Hierarchical clustering successfully captured <strong>distinct food groupings</strong> based on their nutritional attributes.  
                    - The dendrogram provided insights into <strong>nested structures</strong>, revealing how certain food types share similar characteristics at different hierarchical levels.  
                    - This method is particularly useful when we <strong>do not know the exact number of clusters</strong> in advance, allowing for a more flexible approach to categorizing food items.  
                </p>

                <h2>DBSCAN Clustering</h2>

                <div class="gif-section">
                    <img src="assets/dbscan.gif" alt="K-Means Clustering Animation" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
                </div>

                <p class="fade-in">
                    <strong>DBSCAN</strong> (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups data points based on their proximity to each other. Unlike <strong>K-Means</strong> or <strong>Hierarchical Clustering</strong>, DBSCAN does not require specifying the number of clusters beforehand. Instead, it detects dense regions of data and considers low-density points as noise or outliers.
                </p>
                
                <div class="image-container">
                    <img src="visuals/dbscan_clustering.png" alt="DBSCAN Clustering Results" class="topic-image">
                </div>
                
                <p class="fade-in">
                    The visualization above illustrates how <strong>DBSCAN</strong> clusters data in three-dimensional space. Unlike <strong>K-Means</strong>, which assigns every data point to a cluster, DBSCAN allows certain points to remain unclassified if they are too far from any dense cluster. These unclassified points, often labeled as "outliers," are displayed separately in the visualization.
                </p>


                <!-- DBSCAN -->
                
                <h3>DBSCAN Algorithm Parameters</h3>
                <p class="fade-in">
                    DBSCAN operates by defining two critical parameters:
                    <ul>
                        <li><strong>Epsilon (ε)</strong>: The maximum distance between two points for them to be considered in the same neighborhood.</li>
                        <li><strong>Minimum Points (minPts)</strong>: The minimum number of points required to form a dense cluster.</li>
                    </ul>
                </p>
                
                <p class="fade-in">
                    If a point has at least <strong>minPts</strong> neighbors within a radius of <strong>ε</strong>, it becomes a "core point" and expands a cluster. Points that are within <strong>ε</strong> distance of a core point but do not have enough neighbors to form their own cluster are considered "border points." Any point that does not fit into these categories is labeled as noise.
                </p>
                
                <h3>Advantages of DBSCAN</h3>
                <p class="fade-in">
                    <ul>
                        <li><strong>Does not require the number of clusters to be specified</strong>, unlike K-Means.</li>
                        <li><strong>Can identify clusters of arbitrary shape</strong>, which makes it more flexible than K-Means.</li>
                        <li><strong>Effectively detects outliers</strong> as noise, reducing the impact of anomalies on clustering results.</li>
                    </ul>
                </p>
                
                <h3>Limitations of DBSCAN</h3>
                <p class="fade-in">
                    <ul>
                        <li><strong>Sensitive to parameter selection</strong>: The choice of ε and minPts can significantly impact results.</li>
                        <li><strong>Not ideal for varying-density clusters</strong>: When clusters have different densities, DBSCAN might not perform optimally.</li>
                        <li><strong>Computationally expensive for large datasets</strong>: DBSCAN has a higher time complexity compared to K-Means.</li>
                    </ul>
                </p>
                
                <h2>Comparison of Clustering Performance</h2>
                    <p class="fade-in">
                        The <strong>Silhouette Score</strong> was used to evaluate and compare the effectiveness of each clustering method. This metric measures how similar each data point is to its assigned cluster compared to other clusters. A higher <strong>Silhouette Score</strong> indicates better separation between clusters, meaning the clustering method is more effective.
                    </p>
                    
                    <div class="image-container">
                        <img src="visuals/clustering_performance_comparison.png" alt="Clustering Performance Comparison" class="topic-image">
                    </div>
                    
                    <p class="fade-in">
                        The bar chart above provides a direct comparison of <strong>Silhouette Scores</strong> across the three clustering methods:
                        <ul>
                            <li><strong>K-Means</strong> (blue bar) achieved a moderate score, indicating that it formed well-defined clusters, but struggled slightly with irregularly shaped or overlapping clusters.</li>
                            <li><strong>Hierarchical Clustering</strong> (green bar) performed similarly to K-Means, showing strong relationships within clusters but with slightly more flexibility.</li>
                            <li><strong>DBSCAN</strong> (red bar) achieved the highest <strong>Silhouette Score</strong>, making it the <strong>best-performing method</strong> for this dataset.</li>
                        </ul>
                    </p>
    
                    <h3>Comparing Clusters with Original Labels</h3>
                    <p class="fade-in">
                        After clustering, the results were compared to the <strong>original labels</strong> stored before preprocessing. Here are key observations:
                    </p>
                    
                    <ul class="fade-in">
                        <li>Some clusters strongly correspond to original food categories (e.g., high-protein foods forming a distinct cluster).</li>
                        <li>Other clusters highlight hidden relationships between certain food items that were not evident in the labeled dataset.</li>
                    </ul>
                    
                    <h3>Key Observations</h3>
                    <p class="fade-in">
                        <ul>
                            <li><strong>DBSCAN</strong> outperformed the other methods, demonstrating its ability to detect clusters of varying shapes and densities while effectively handling noise and outliers.</li>
                            <li><strong>K-Means</strong> and <strong>Hierarchical Clustering</strong> produced similar scores, indicating that while both methods structured the data well, they may have struggled with non-spherical clusters or varying densities.</li>
                            <li>The high score of <strong>DBSCAN</strong> suggests that the dataset contains complex cluster structures that do not conform to the strict assumptions made by <strong>K-Means</strong> and <strong>Hierarchical Clustering</strong>.</li>
                        </ul>
                    </p>

                    <h2>Conclusion</h2>

                    <p class="fade-in">
                        Understanding <strong>food composition</strong> is crucial for making informed dietary choices. By utilizing clustering techniques, we can uncover <strong>hidden patterns</strong> within food data that provide valuable insights into <strong>nutritional similarities and differences</strong>. These insights can help in <strong>designing healthier meal plans</strong>, improving food recommendations, and identifying potentially misleading food classifications.
                    </p>
                    
                    <p class="fade-in">
                        The study of food clustering showcases how different items relate to one another based on their <strong>macronutrient distribution</strong>. Traditional methods of categorizing food by name or brand often fail to reflect their true nutritional composition. However, through clustering, we can <strong>identify groups of foods that share similar characteristics</strong>, regardless of how they are marketed or labeled.
                    </p>
                    
                    <!-- Clustering Conclusion Image -->
                    <div class="image-container">
                        <img src="assets/clustering_conclusion.png" alt="Clustering Conclusion Insights" class="topic-image">
                    </div>
                    
                    <p class="fade-in">
                        Moreover, this approach enables <strong>better consumer awareness</strong>. For example, individuals looking to <strong>reduce sugar intake</strong> can use these clusters to find alternatives that match their dietary goals without being misled by branding. Similarly, athletes or those focusing on <strong>high-protein diets</strong> can identify food groups that meet their nutritional needs more effectively.
                    </p>
                    
                    <p class="fade-in">
                        From a broader perspective, <strong>clustering techniques in food data analysis</strong> have the potential to assist policymakers in <strong>creating better food labeling regulations</strong>, improving <strong>public health nutrition strategies</strong>, and even addressing concerns about <strong>ultra-processed food consumption</strong>. As food choices continue to evolve, <strong>data-driven methods</strong> like clustering offer a pathway to a <strong>deeper, more accurate understanding</strong> of what we consume and how it impacts our well-being.
                    </p>
        </div>
    </div>
    <!-- Typing Animation -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            let text = "Clustering";
            let i = 0;
            function typeWriter() {
                if (i < text.length) {
                    document.getElementById("typing-title").innerHTML += text.charAt(i);
                    i++;
                    setTimeout(typeWriter, 50);
                }
            }
            typeWriter();
        });
    </script>

    <!-- Scroll-Based Fade-In Effect -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            let fadeInElements = document.querySelectorAll(".fade-in");

            function fadeInOnScroll() {
                fadeInElements.forEach(element => {
                    let position = element.getBoundingClientRect().top;
                    let screenHeight = window.innerHeight;

                    if (position < screenHeight - 100) {
                        element.classList.add("visible");
                    }
                });
            }

            window.addEventListener("scroll", fadeInOnScroll);
            fadeInOnScroll();
        });
    </script>

    <script>
        function toggleMenu() {
            var navbar = document.querySelector(".navbar");
            navbar.classList.toggle("active");
        }
    </script>
</body>
</html>