<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Naive Bayes</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="food-background"></div>
    <div class="light-overlay"></div>

    <div class="menu-icon" onclick="toggleMenu()">‚ò∞</div>
    <div class="navbar">
        <a href="index.html">Home</a>
        <a href="introduction.html">Introduction</a>
        <a href="data_prep.html">Data Prep</a>
        <a href="eda.html">EDA</a>
        <a href="pca.html">PCA</a>
        <a href="clustering.html">Clustering</a>
        <a href="arm.html">ARM</a>
        <a href="decision_trees.html">Decision Trees</a>
        <a href="naive_bayes.html" class="active">Naive Bayes</a>
        <a href="svm.html">SVM</a>
        <a href="regression.html">Regression</a>
        <a href="conclusions.html">Conclusions</a>
        <a href="about_me.html">About Me</a>
    </div>

    <div class="content-section">
        <h1 id="typing-title"></h1>
        <div class="content-wrapper">
            
            <h2>Introduction to Na√Øve Bayes for Food Classification</h2>
            <p class="fade-in">
                Na√Øve Bayes (NB) is a family of probabilistic classifiers grounded in Bayes‚Äô Theorem. Despite assuming feature independence, these models often outperform more sophisticated algorithms on small to medium-sized datasets. NB classifiers are particularly effective in text classification, spam filtering, sentiment analysis, and here‚Äînutritional food classification.
            </p>
            
            <p class="fade-in">
                The equation below illustrates the fundamental principle of Na√Øve Bayes: it calculates the posterior probability of a class given the input features by combining prior knowledge and likelihood under the assumption of feature independence.
            </p>
            
            <div class="image-container">
                <img src="assets/naive_bayes_formula.png" alt="Naive Bayes Formula" class="topic-image">
            </div>

            <h2>Variants of Na√Øve Bayes Explored</h2>

            <div class="image-container">
                <img src="assets/nb_types.jpg" alt="Naive Bayes Formula" class="topic-image">
            </div>

            <ul class="fade-in">
                <li><strong>Gaussian Na√Øve Bayes (GNB)</strong>: Assumes continuous features follow a Gaussian distribution. Suitable for numeric data like fat, sugar, or protein grams.</li>
                <li><strong>Multinomial Na√Øve Bayes (MNB)</strong>: Models count-based or frequency features. Applied to scaled and integer-rounded nutrition values.</li>
                <li><strong>Bernoulli Na√Øve Bayes (BNB)</strong>: Ideal for binary data. We binarized nutrient presence/absence.</li>
                <li><strong>Categorical Na√Øve Bayes (CNB)</strong>: Deals with discrete categories. Used <code>KBinsDiscretizer</code> to convert continuous values into bins.</li>
            </ul>

            <h2>Data Preparation</h2>
            <p class="fade-in">
                Our input dataset, <code>clean_normalized_data.csv</code>, was divided into stratified training (70%) and testing (30%) sets. Different preprocessing strategies were applied:
            </p>

            <div class="image-container">
                <img src="assets/clean_normalized_data.png" alt="Clean Normalized Data Preview" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/clean_normalized_data.csv" target="_blank" class="github-button">View Dataset</a>
            </div>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/nb_data_prep.py" target="_blank" class="github-button">Naive Bayes Script</a>
            </div>

            <ul class="fade-in">
                <li><strong>GNB</strong>: Used continuous normalized data directly.</li>
                <li><strong>MNB</strong>: Applied MinMax scaling ‚Üí multiplied by 100 ‚Üí converted to integers.</li>
                <li><strong>BNB</strong>: Binarized data with thresholding (value &gt; 0 = 1, else 0).</li>
                <li><strong>CNB</strong>: Used KBinsDiscretizer to convert features into 10 categorical bins.</li>
            </ul>

            <h2>Model Codebase</h2>
            <p class="fade-in">
                Each Na√Øve Bayes model in this project was implemented through a dedicated Python script. These scripts are tailored to the assumptions and requirements of each model type‚ÄîGaussian, Multinomial, Bernoulli, and Categorical. All models build upon a unified preprocessing step to ensure clean, well-structured data. Below is a breakdown of each modeling component and its role in this project.
            </p>
            
            <!-- Data Preparation -->
            <h3 class="fade-in">üì¶ Data Preparation Script</h3>
            <p class="fade-in">
                The preprocessing workflow loads the cleaned USDA dataset and prepares data in four specific formats. It filters out underrepresented categories, applies a stratified 70-30 train-test split, and tailors the features to match each NB variant:
            </p>
            <ul class="fade-in">
                <li>Continuous features for Gaussian NB</li>
                <li>Scaled, count-like integers for Multinomial NB</li>
                <li>Binary presence/absence indicators for Bernoulli NB</li>
                <li>Discretized categorical bins for Categorical NB</li>
            </ul>
            
            <!-- Gaussian Naive Bayes -->
            <h3 class="fade-in">üìä Gaussian Na√Øve Bayes (GNB)</h3>
            <p class="fade-in">
                This model was trained using raw continuous features like calories, fat, protein, and carbohydrate ratios. The goal was to assess how well GNB could classify foods when the input distribution is assumed to be Gaussian. The model demonstrated strong separation in well-behaved, unimodal classes (like "Soda"), but struggled when class boundaries overlapped or distributions were skewed, as seen in categories like "Pizza" and "Frozen Patties."
            </p>

            <!-- Dataset Explanation -->
            <h4 class="fade-in">üß™ GNB Dataset Split and Why Disjoint Sets Are Crucial</h4>
            <p class="fade-in">
                For effective supervised learning, the dataset was split into a <strong>training set</strong> and a <strong>testing set</strong> using a 70:30 stratified split. This ensures that the classifier learns on a subset of labeled data and is then evaluated on previously unseen examples, offering a genuine test of generalization. 
                Disjoint sets are essential because reusing data between training and testing can lead to <strong>data leakage</strong>, where the model memorizes instead of learning general patterns. This results in overestimated accuracy and poor real-world performance. Below are actual screenshots of the prepared GNB feature matrices and target labels.
            </p>

            <!-- X Train -->
            <h4 class="fade-in">üìÅ GNB_X_Train: Feature matrix used for training</h4>
            <div class="image-container">
                <img src="assets/GNB_X_Train.jpg" alt="GNB X Train Dataset" class="topic-image" style="max-width: 100%; border-radius: 8px;">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_GNB_X_train.csv" target="_blank" class="github-button">View GNB_X_Train.csv</a>
            </div>

            <!-- Y Train -->
            <h4 class="fade-in">üè∑Ô∏è GNB_Y_Train: Corresponding food categories for training set</h4>
            <div class="image-container">
                <img src="assets/GNB_Y_Train.jpg" alt="GNB Y Train Labels" class="topic-image" style="max-width: 100%; border-radius: 8px;">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_GNB_y_train.csv" target="_blank" class="github-button">View GNB_Y_Train.csv</a>
            </div>

            <!-- X Test -->
            <h4 class="fade-in">üìÅ GNB_X_Test: Feature matrix used for evaluating performance</h4>
            <div class="image-container">
                <img src="assets/GNB_X_Test.jpg" alt="GNB X Test Dataset" class="topic-image" style="max-width: 100%; border-radius: 8px;">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_GNB_X_test.csv" target="_blank" class="github-button">View GNB_X_Test.csv</a>
            </div>

            <!-- Y Test -->
            <h4 class="fade-in">üè∑Ô∏è GNB_Y_Test: True food labels used to validate predictions</h4>
            <div class="image-container">
                <img src="assets/GNB_Y_Test.jpg" alt="GNB Y Test Labels" class="topic-image" style="max-width: 100%; border-radius: 8px;">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_GNB_y_test.csv" target="_blank" class="github-button">View GNB_Y_Test.csv</a>
            </div>
            
            <!-- Multinomial Naive Bayes -->
            <h3 class="fade-in">üìà Multinomial Na√Øve Bayes (MNB)</h3>
            <p class="fade-in">
                This implementation used MinMax-scaled nutritional data converted into non-negative integer-like values by multiplying and rounding. This transformation preserved the relative frequencies of features such as calories, protein, fat, and carbohydrate ratios‚Äîmaking the dataset well-suited for frequency-based classifiers like MNB.
            </p>
            <p class="fade-in">
                The training and test sets were created using a stratified 70/30 split to maintain balanced representation across food categories. Ensuring these sets are <strong>disjoint</strong> is critical: it prevents data leakage, which would inflate performance metrics by allowing the model to "memorize" parts of the test set during training. Below are small snapshots of the <code>X_train</code> and <code>X_test</code> datasets used for MNB.
            </p>

            <div class="image-container">
                <img src="assets/MNB_X_Train.jpg" alt="MNB Training Set (X_train)" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_MNB_X_train.csv" target="_blank" class="github-button">View MNB_X_Train.csv</a>
            </div>

            <div class="image-container">
                <img src="assets/MNB_X_Test.jpg" alt="MNB Test Set (X_test)" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_MNB_X_test.csv" target="_blank" class="github-button">View MNB_X_Test.csv</a>
            </div>

            <p class="fade-in">
                As seen above, both datasets contain the same feature columns but different samples‚Äîensuring that evaluation is performed fairly on unseen data. The MNB classifier trained on these disjoint sets performed exceptionally well on classes with strong, distinct patterns like <strong>Pizza</strong> and <strong>Candy</strong>. This highlights the strength of MNB when applied to structured, integer-based nutritional data where feature frequencies act as strong class signals.
            </p>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/nb_mnb.py" target="_blank" class="github-button">View MNB Model Script</a>
            </div>
            
            <!-- Bernoulli Naive Bayes -->
            <h3 class="fade-in">‚ö™ Bernoulli Na√Øve Bayes (BNB)</h3>
            <p class="fade-in">
                Bernoulli Na√Øve Bayes is best suited for binary/boolean feature data‚Äîwhere each feature is either present (1) or absent (0). For this implementation, all continuous nutritional values were converted into binary flags using a threshold: if a nutrient was greater than zero, it was encoded as 1; otherwise, it was marked as 0.
            </p>
            <p class="fade-in">
                This model is particularly useful when the presence of a feature matters more than its magnitude. In our food dataset, it performed well in distinguishing categories like <strong>‚ÄúCandy‚Äù</strong> and <strong>‚ÄúCookies & Biscuits‚Äù</strong>, where specific nutrients (e.g., sugar, fat) are either consistently present or absent.
            </p>
            <p class="fade-in">
                However, Bernoulli NB comes with significant limitations when applied to nuanced datasets like nutrition profiles. By binarizing the data, we lose valuable detail‚Äîsuch as how much sugar or fat is present. This leads to overclassification, where many distinct items get mapped to the same label (e.g., ‚ÄúCookies‚Äù) simply because they share similar binary nutrient presence. As a result, categories with overlapping 0/1 patterns were frequently misclassified, reducing the model‚Äôs overall accuracy.
            </p>
            <p class="fade-in">
                Below are snapshots of the training and testing datasets used for BNB. As shown, the data has been reduced to binary format, discarding numeric ranges in favor of presence/absence indicators. This format aligns with BNB‚Äôs assumptions but also introduces a trade-off between simplicity and predictive resolution.
            </p>
            
            <div class="image-container fade-in">
                <img src="assets/BNB_X_Train.jpg" alt="BNB Training Set" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_BNB_X_train.csv" target="_blank" class="github-button">View BNB_X_Train.csv</a>
            </div>
            
            <div class="image-container fade-in">
                <img src="assets/BNB_X_Test.jpg" alt="BNB Testing Set" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_BNB_X_test.csv" target="_blank" class="github-button">View BNB_X_Test.csv</a>
            </div>

            
            <!-- Categorical Naive Bayes -->
            <h3 class="fade-in">üß© Categorical Na√Øve Bayes (CNB)</h3>
            <p class="fade-in">
                Categorical Na√Øve Bayes is specifically designed to handle discrete, unordered categories‚Äîmaking it appropriate for cases like customer types, weather labels, or survey responses. However, since our nutritional data was continuous in nature, we applied <code>KBinsDiscretizer</code> to convert each feature into 10 equally spaced bins. This transformation effectively grouped nutrient values (e.g., calories, fat) into distinct integer labels, simulating a categorical structure.
            </p>
            <p class="fade-in">
                The CNB model was trained and tested using the exact same data splits used for Gaussian Na√Øve Bayes (GNB)‚Äîspecifically, <code>nb_GNB_X_train.csv</code> and <code>nb_GNB_X_test.csv</code>‚Äîbut with each feature bin-assigned to fall within a discrete category. This ensured a consistent comparison across models while adapting the data format to suit CNB‚Äôs assumptions.
            </p>

                        <!-- X Train -->
                        <h4 class="fade-in">üìÅ GNB_X_Train: Feature matrix used for training</h4>
                        <div class="image-container">
                            <img src="assets/GNB_X_Train.jpg" alt="GNB X Train Dataset" class="topic-image" style="max-width: 100%; border-radius: 8px;">
                        </div>
                        <div class="button-section">
                            <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_GNB_X_train.csv" target="_blank" class="github-button">View GNB_X_Train.csv</a>
                        </div> 

                        <!-- X Test -->
                        <h4 class="fade-in">üìÅ GNB_X_Test: Feature matrix used for evaluating performance</h4>
                        <div class="image-container">
                            <img src="assets/GNB_X_Test.jpg" alt="GNB X Test Dataset" class="topic-image" style="max-width: 100%; border-radius: 8px;">
                        </div>
                        <div class="button-section">
                            <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_GNB_X_test.csv" target="_blank" class="github-button">View GNB_X_Test.csv</a>
                        </div>

            <p class="fade-in">
                While the conversion allowed us to apply CNB, the results were suboptimal. The discretization process introduced distortion by forcing continuous values into arbitrary bins, removing the natural ordering and subtle differences between data points. As a result, the model consistently overpredicted ‚ÄúSoda‚Äù as the target class, failing to distinguish most other food types. This behavior highlights the risks of applying CNB to datasets where numeric ranges hold significant meaning‚Äîespecially in contexts like nutrient-based classification where gradations in sugar, fat, or protein are vital.
            </p>
            <p class="fade-in">
                Nonetheless, CNB served as a valuable contrast to the other Na√Øve Bayes flavors by illustrating how binning affects model behavior. Its poor performance underscores the importance of aligning preprocessing strategies with the assumptions of the model being used.
            </p>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/nb_cnb.py" target="_blank" class="github-button">View CNB Model Script</a>
            </div>

            <h2>Confusion Matrix Analysis</h2>

            <h3>Gaussian Na√Øve Bayes (GNB)</h3>
            <div class="image-container">
                <img src="visuals/GNB_confusion_matrix_top10_rich.png" alt="GNB Confusion Matrix" class="topic-image">
            </div>
            <p class="fade-in"><em>
                ‚úÖ GNB classified "Soda" and "Cookies & Biscuits" reliably, but struggled with overlapping nutrient profiles like those in "Pizza" and "Frozen Patties."
            </em></p>
            
            <p class="fade-in">
                The GNB classifier struggled with significant class overlap, largely due to its core assumption that features are normally (Gaussian) distributed and independent given the class label...
            </p>
            
            <h3>Multinomial Na√Øve Bayes (MNB)</h3>
            <div class="image-container">
                <img src="visuals/MNB_confusion_matrix_top10_rich.png" alt="MNB Confusion Matrix" class="topic-image">
            </div>
            <p class="fade-in"><em>
                ‚úÖ MNB showed excellent performance on "Pizza" and "Candy", indicating strong separation with count-like nutritional patterns.
            </em></p>
            
            <p class="fade-in">
                The Multinomial Na√Øve Bayes (MNB) classifier demonstrated strong performance across multiple food categories...
            </p>
            
            <h3>Bernoulli Na√Øve Bayes (BNB)</h3>
            <div class="image-container">
                <img src="visuals/BNB_confusion_matrix_top10.png" alt="BNB Confusion Matrix" class="topic-image">
            </div>
            <p class="fade-in"><em>
                ‚úÖ BNB accurately classified "Candy" and "Cookies & Biscuits" using binary nutrient flags, but struggled to differentiate other categories due to oversimplification.
            </em></p>
            
            <p class="fade-in">
                The Bernoulli Na√Øve Bayes (BNB) model yielded notably high classification accuracy for categories such as "Cookies & Biscuits" and "Candy"...
            </p>
            
            <h3>Categorical Na√Øve Bayes (CNB)</h3>
            <div class="image-container">
                <img src="visuals/CNB_confusion_matrix_top10.png" alt="CNB Confusion Matrix" class="topic-image">
            </div>
            <p class="fade-in"><em>
                ‚ö†Ô∏è CNB misclassified most categories as "Soda", revealing its incompatibility with discretized continuous data in this context.
            </em></p>
            
            <p class="fade-in">
                The Categorical Na√Øve Bayes (CNB) model struggled significantly in this task, with over 90% of samples from diverse categories being misclassified as ‚ÄúSoda.‚Äù...
            </p>

            <h3 class="fade-in">Model Accuracy Snapshots</h3>
            <p class="fade-in">
                Below are final accuracy snapshots for each Naive Bayes variant trained using the top-10 most frequent food categories.
                Each model type uses a unique assumption about feature distributions, leading to varying prediction performance.
                The classification reports highlight class-wise precision, recall, and F1-scores, helping assess the suitability of each approach.
            </p>

            <!-- Gaussian Naive Bayes -->
            <div class="image-container">
                <img src="assets/gnb_accuracy.png" alt="Gaussian Naive Bayes Accuracy" class="topic-image">
            </div>
            <p class="fade-in">
                <strong>Gaussian Naive Bayes (GNB)</strong> assumes features follow a normal distribution. Despite this, the model achieved a modest accuracy of <strong>4.11%</strong>‚Äîsuggesting that the continuous-valued nutritional data does not align well with Gaussian assumptions.
                However, certain classes like <em>Baby food: vegetables</em> (Precision: 0.07, Recall: 0.86) and <em>Baked Products</em> (Precision: 0.19, Recall: 0.27) were identified reasonably well.
                This model exposes how continuous features can dilute predictive power when distributions are non-Gaussian.
            </p>

            <!-- Multinomial Naive Bayes -->
            <div class="image-container">
                <img src="assets/mnb_accuracy.png" alt="Multinomial Naive Bayes Accuracy" class="topic-image">
            </div>
            <p class="fade-in">
                <strong>Multinomial Naive Bayes (MNB)</strong> works well with count-like features. After preprocessing and normalization, this model yielded an accuracy of <strong>29.11%</strong>.
                Standout performance includes <em>Bacon, Sausages & Ribs</em> with high precision (0.67) and <em>Breads & Buns</em> with strong recall (0.59).
                The model struggled with diverse and ambiguous food items like <em>Biscuits</em> and <em>Cookies</em>, likely due to overlapping nutritional profiles.
                Overall, MNB offered balanced performance on the most frequent food categories and proved useful in this discrete feature setting.
            </p>

            <!-- Bernoulli Naive Bayes -->
            <div class="image-container">
                <img src="assets/bnb_accuracy.png" alt="Bernoulli Naive Bayes Accuracy" class="topic-image">
            </div>
            <p class="fade-in">
                <strong>Bernoulli Naive Bayes (BNB)</strong> simplifies input to binary form‚Äîindicating the presence or absence of features.
                It delivered a fair accuracy of <strong>35.15%</strong>, outperforming Gaussian and Categorical variants.
                The best predictions were for <em>American Indian/Alaska Native Foods</em> (Precision: 0.52, Recall: 0.13).
                BNB is well-suited when feature occurrence is more informative than its magnitude, but can miss nuance in varied nutritional scales.
            </p>

            <!-- Categorical Naive Bayes -->
            <div class="image-container">
                <img src="assets/cnb_accuracy.png" alt="Categorical Naive Bayes Accuracy" class="topic-image">
            </div>
            <p class="fade-in">
                <strong>Categorical Naive Bayes (CNB)</strong> uses discretized numerical features converted into categorical bins.
                It had the lowest accuracy among all models at <strong>15.78%</strong>.
                This poor performance likely stems from information loss during binning and a mismatch between discretized values and true class boundaries.
                While theoretically appealing, CNB requires precise bin tuning to be effective‚Äîespecially when features like calories and fat span wide ranges.
            </p>

            <h2>Conclusions & Takeaways</h2>
            <ul class="fade-in">
                <li>
                    <strong>Multinomial Na√Øve Bayes (MNB) delivered the most robust performance</strong> across the board. This model‚Äôs effectiveness stems from its ability to model feature frequencies or counts‚Äîclosely resembling the structure of scaled nutritional values like grams of protein, sugar, or fat. Even after converting values to non-negative integers, MNB retained enough numerical resolution to differentiate between subtle food category distinctions. Its classification accuracy for diverse categories such as "Pizza" and "Candy" suggests MNB adapts well to structured, quantity-driven food profiles, making it highly appropriate for datasets based on nutrient-based features.
                </li>
            
                <li>
                    <strong>Bernoulli Na√Øve Bayes (BNB) performed moderately well but oversimplified the dataset.</strong> BNB is ideal for binary classification tasks‚Äîsuch as spam detection or presence/absence indicators‚Äîbut applying it to continuous nutrition data required aggressive binarization. This transformation led to a significant loss of detail. Despite some strong predictions (notably for high-signal categories like ‚ÄúCandy‚Äù), the model frequently misclassified items as "Cookies & Biscuits" due to dominant 1/0 patterns across features. While useful for highlighting presence-based characteristics, BNB lacks the nuance to capture gradual nutritional differences.
                </li>
            
                <li>
                    <strong>Gaussian Na√Øve Bayes (GNB) offered mixed results.</strong> While it operates under the assumption that features are normally distributed, most real-world food data does not adhere to a single-mode bell curve. This mismatch was evident in the model‚Äôs confusion matrices‚Äîonly categories with relatively tight distributions like ‚ÄúSoda‚Äù and ‚ÄúBreads & Buns‚Äù were consistently classified well. More complex or overlapping categories (e.g., ‚ÄúPizza,‚Äù ‚ÄúFrozen Patties‚Äù) led to heavy confusion. GNB might be viable in datasets where features have clean Gaussian distributions, but for diverse food classes, it lacks adaptability.
                </li>
            
                <li>
                    <strong>Categorical Na√Øve Bayes (CNB) performed the weakest overall</strong> due to its fundamental incompatibility with continuous data. The process of binning numerical features into discrete categories using <code>KBinsDiscretizer</code> severely degraded the information content, compressing subtle distinctions into broad buckets. As a result, CNB misclassified over 90% of food items as ‚ÄúSoda,‚Äù revealing how categorical encodings can unintentionally create artificial similarities. CNB is best reserved for datasets where features are naturally nominal‚Äîsuch as flavor type, brand name, or food category‚Äînot scaled nutrient quantities.
                </li>
            </ul>

            <p class="fade-in">
                Overall, Na√Øve Bayes proved efficient and scalable. Despite the feature independence assumption, performance was solid. For real-world food classification, MNB is recommended for count/frequency data, and GNB where distributions align with Gaussian assumptions.
            </p>
        </div>
    </div>




            <!-- Your existing HTML content -->

            <script>
                document.addEventListener("DOMContentLoaded", function() {
                    let text = "Naive Bayes";
                    let i = 0;
                    function typeWriter() {
                        if (i < text.length) {
                            document.getElementById("typing-title").innerHTML += text.charAt(i);
                            i++;
                            setTimeout(typeWriter, 50);
                        }
                    }
                    typeWriter();
                });
            </script>

        <!-- üîπ JavaScript for Scroll-Based Fade-In Effect -->
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                let fadeInElements = document.querySelectorAll(".fade-in");
    
                function fadeInOnScroll() {
                    fadeInElements.forEach(element => {
                        let position = element.getBoundingClientRect().top;
                        let screenHeight = window.innerHeight;
    
                        if (position < screenHeight - 100) {
                            element.classList.add("visible");
                        }
                    });
                }
    
                window.addEventListener("scroll", fadeInOnScroll);
                fadeInOnScroll(); // Trigger on load
            });
        </script>

    <script>
        function toggleMenu() {
            var navbar = document.querySelector(".navbar");
            navbar.classList.toggle("active");
        }
    </script>
    <script type="module" src="animations.js"></script>
    <div class="transition-overlay"></div>
    <script type="module" src="animations.js"></script>
</body>
</html>