<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Naive Bayes</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="food-background"></div>
    <div class="light-overlay"></div>

    <div class="menu-icon" onclick="toggleMenu()">‚ò∞</div>
    <div class="navbar">
        <a href="index.html">Home</a>
        <a href="introduction.html">Introduction</a>
        <a href="data_prep.html">Data Prep</a>
        <a href="eda.html">EDA</a>
        <a href="pca.html">PCA</a>
        <a href="clustering.html">Clustering</a>
        <a href="arm.html">ARM</a>
        <a href="decision_trees.html">Decision Trees</a>
        <a href="naive_bayes.html" class="active">Naive Bayes</a>
        <a href="svm.html">SVM</a>
        <a href="regression.html">Regression</a>
        <a href="conclusions.html">Conclusions</a>
        <a href="about_me.html">About Me</a>
    </div>

    <div class="content-section">
        <h1 id="typing-title"></h1>
        <div class="content-wrapper">
            
            <h2>Introduction to Na√Øve Bayes for Food Classification</h2>
            <p class="fade-in">
                Na√Øve Bayes (NB) is a family of probabilistic classifiers grounded in Bayes‚Äô Theorem. Despite assuming feature independence, these models often outperform more sophisticated algorithms on small to medium-sized datasets. NB classifiers are particularly effective in text classification, spam filtering, sentiment analysis, and here‚Äînutritional food classification.
            </p>

            <div class="image-container">
                <img src="assets/naive_bayes_formula.png" alt="Naive Bayes Formula" class="topic-image">
            </div>

            <h2>Variants of Na√Øve Bayes Explored</h2>
            <ul class="fade-in">
                <li><strong>Gaussian Na√Øve Bayes (GNB)</strong>: Assumes continuous features follow a Gaussian distribution. Suitable for numeric data like fat, sugar, or protein grams.</li>
                <li><strong>Multinomial Na√Øve Bayes (MNB)</strong>: Models count-based or frequency features. Applied to scaled and integer-rounded nutrition values.</li>
                <li><strong>Bernoulli Na√Øve Bayes (BNB)</strong>: Ideal for binary data. We binarized nutrient presence/absence.</li>
                <li><strong>Categorical Na√Øve Bayes (CNB)</strong>: Deals with discrete categories. Used <code>KBinsDiscretizer</code> to convert continuous values into bins.</li>
            </ul>

            <h2>Data Preparation</h2>
            <p class="fade-in">
                Our input dataset, <code>clean_normalized_data.csv</code>, was divided into stratified training (70%) and testing (30%) sets. Different preprocessing strategies were applied:
            </p>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/nb_data_prep.py" target="_blank" class="github-button">Naive Bayes Script</a>
            </div>

            <ul class="fade-in">
                <li><strong>GNB</strong>: Used continuous normalized data directly.</li>
                <li><strong>MNB</strong>: Applied MinMax scaling ‚Üí multiplied by 100 ‚Üí converted to integers.</li>
                <li><strong>BNB</strong>: Binarized data with thresholding (value &gt; 0 = 1, else 0).</li>
                <li><strong>CNB</strong>: Used KBinsDiscretizer to convert features into 10 categorical bins.</li>
            </ul>

            <div class="image-container">
                <img src="assets/nb_data.jpg" alt="Sample of prepared Naive Bayes dataset" class="topic-image">
            </div>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_BNB_X_test.csv" target="_blank" class="github-button">Naive Bayes Datasets</a>
            </div>

            <h2>Model Codebase</h2>
            <p class="fade-in">All Na√Øve Bayes models were coded in separate scripts with shared preprocessing logic in <code>nb_data_prep.py</code>. The complete implementation includes:</p>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/nb_gnb.py" target="_blank" class="github-button">GNB Model Script</a>
            </div>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/nb_mnb.py" target="_blank" class="github-button">MNB Model Script</a>
            </div>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/bnb_model.py" target="_blank" class="github-button">BNB Model Script</a>
            </div>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/nb_cnb.py" target="_blank" class="github-button">CNB Model Script</a>
            </div>

            <h2>Confusion Matrix Analysis</h2>
            <h3>Gaussian Na√Øve Bayes (GNB)</h3>
            <div class="image-container">
                <img src="visuals/GNB_confusion_matrix_top10_rich.png" alt="GNB Confusion Matrix" class="topic-image">
            </div>
            <p class="fade-in">
                The GNB classifier struggled with class overlap due to violations of the normality assumption. Only ‚ÄúSoda,‚Äù ‚ÄúCookies & Biscuits,‚Äù and ‚ÄúBreads & Buns‚Äù saw clean separation. Others like ‚ÄúFrozen Patties‚Äù and ‚ÄúPizza‚Äù were frequently misclassified, indicating GNB's limitations in food classification.
            </p>

            <h3>Multinomial Na√Øve Bayes (MNB)</h3>
            <div class="image-container">
                <img src="visuals/MNB_confusion_matrix_top10_rich.png" alt="MNB Confusion Matrix" class="topic-image">
            </div>
            <p class="fade-in">
                MNB performed exceptionally well on ‚ÄúPizza‚Äù (92% accuracy) and ‚ÄúCandy‚Äù (68%). It struggled to distinguish ‚ÄúCookies & Biscuits‚Äù from ‚ÄúIce Cream & Frozen Yogurt,‚Äù indicating shared nutritional patterns. Best overall classifier across balanced classes.
            </p>

            <h3>Bernoulli Na√Øve Bayes (BNB)</h3>
            <div class="image-container">
                <img src="visuals/BNB_confusion_matrix_top10.png" alt="BNB Confusion Matrix" class="topic-image">
            </div>
            <p class="fade-in">
                BNB showed strong performance on ‚ÄúCookies & Biscuits‚Äù and ‚ÄúCandy‚Äù (88%+), but overclassified several categories as ‚ÄúCookies.‚Äù Binarizing nutrition data led to feature loss, reducing nuance and fine-grained prediction.
            </p>

            <h3>Categorical Na√Øve Bayes (CNB)</h3>
            <div class="image-container">
                <img src="visuals/CNB_confusion_matrix_top10.png" alt="CNB Confusion Matrix" class="topic-image">
            </div>
            <p class="fade-in">
                CNB suffered from major misclassifications. Over 90% of items from multiple categories were wrongly labeled as ‚ÄúSoda.‚Äù The binning strategy in CNB likely distorted original distributions. Useful only if original data is naturally categorical.
            </p>

            <h2>Conclusions & Takeaways</h2>
            <ul class="fade-in">
                <li><strong>MNB outperformed all models</strong> on balanced and non-binary food data due to its handling of counts.</li>
                <li><strong>BNB</strong> was reliable for binary labels but too simplistic for nutritional ranges.</li>
                <li><strong>GNB</strong> worked well on few normally distributed features but failed on multi-modal food groups.</li>
                <li><strong>CNB</strong> is not suitable unless features are discrete and unordered by nature.</li>
            </ul>

            <p class="fade-in">
                Overall, Na√Øve Bayes proved efficient and scalable. Despite the feature independence assumption, performance was solid. For real-world food classification, MNB is recommended for count/frequency data, and GNB where distributions align with Gaussian assumptions.
            </p>
        </div>
    </div>




            <!-- Your existing HTML content -->

            <script>
                document.addEventListener("DOMContentLoaded", function() {
                    let text = "Naive Bayes";
                    let i = 0;
                    function typeWriter() {
                        if (i < text.length) {
                            document.getElementById("typing-title").innerHTML += text.charAt(i);
                            i++;
                            setTimeout(typeWriter, 50);
                        }
                    }
                    typeWriter();
                });
            </script>

        <!-- üîπ JavaScript for Scroll-Based Fade-In Effect -->
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                let fadeInElements = document.querySelectorAll(".fade-in");
    
                function fadeInOnScroll() {
                    fadeInElements.forEach(element => {
                        let position = element.getBoundingClientRect().top;
                        let screenHeight = window.innerHeight;
    
                        if (position < screenHeight - 100) {
                            element.classList.add("visible");
                        }
                    });
                }
    
                window.addEventListener("scroll", fadeInOnScroll);
                fadeInOnScroll(); // Trigger on load
            });
        </script>

    <script>
        function toggleMenu() {
            var navbar = document.querySelector(".navbar");
            navbar.classList.toggle("active");
        }
    </script>
    <script type="module" src="animations.js"></script>
    <div class="transition-overlay"></div>
    <script type="module" src="animations.js"></script>
</body>
</html>