<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Naive Bayes</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="food-background"></div>
    <div class="light-overlay"></div>

    <div class="menu-icon" onclick="toggleMenu()">‚ò∞</div>
    <div class="navbar">
        <a href="index.html">Home</a>
        <a href="introduction.html">Introduction</a>
        <a href="data_prep.html">Data Prep</a>
        <a href="eda.html">EDA</a>
        <a href="pca.html">PCA</a>
        <a href="clustering.html">Clustering</a>
        <a href="arm.html">ARM</a>
        <a href="decision_trees.html">Decision Trees</a>
        <a href="naive_bayes.html" class="active">Naive Bayes</a>
        <a href="svm.html">SVM</a>
        <a href="regression.html">Regression</a>
        <a href="conclusions.html">Conclusions</a>
        <a href="about_me.html">About Me</a>
    </div>

    <div class="content-section">
        <h1 id="typing-title"></h1>
        <div class="content-wrapper">
            
            <h2>Introduction to Na√Øve Bayes for Food Classification</h2>
            <p class="fade-in">
                Na√Øve Bayes (NB) is a family of probabilistic classifiers grounded in Bayes‚Äô Theorem. Despite assuming feature independence, these models often outperform more sophisticated algorithms on small to medium-sized datasets. NB classifiers are particularly effective in text classification, spam filtering, sentiment analysis, and here‚Äînutritional food classification.
            </p>
            
            <p class="fade-in">
                The equation below illustrates the fundamental principle of Na√Øve Bayes: it calculates the posterior probability of a class given the input features by combining prior knowledge and likelihood under the assumption of feature independence.
            </p>
            
            <div class="image-container">
                <img src="assets/naive_bayes_formula.png" alt="Naive Bayes Formula" class="topic-image">
            </div>

            <h2>Variants of Na√Øve Bayes Explored</h2>

            <div class="image-container">
                <img src="assets/nb_types.jpg" alt="Naive Bayes Formula" class="topic-image">
            </div>

            <ul class="fade-in">
                <li><strong>Gaussian Na√Øve Bayes (GNB)</strong>: Assumes continuous features follow a Gaussian distribution. Suitable for numeric data like fat, sugar, or protein grams.</li>
                <li><strong>Multinomial Na√Øve Bayes (MNB)</strong>: Models count-based or frequency features. Applied to scaled and integer-rounded nutrition values.</li>
                <li><strong>Bernoulli Na√Øve Bayes (BNB)</strong>: Ideal for binary data. We binarized nutrient presence/absence.</li>
                <li><strong>Categorical Na√Øve Bayes (CNB)</strong>: Deals with discrete categories. Used <code>KBinsDiscretizer</code> to convert continuous values into bins.</li>
            </ul>

            <h2>Data Preparation</h2>
            <p class="fade-in">
                Our input dataset, <code>clean_normalized_data.csv</code>, was divided into stratified training (70%) and testing (30%) sets. Different preprocessing strategies were applied:
            </p>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/nb_data_prep.py" target="_blank" class="github-button">Naive Bayes Script</a>
            </div>

            <ul class="fade-in">
                <li><strong>GNB</strong>: Used continuous normalized data directly.</li>
                <li><strong>MNB</strong>: Applied MinMax scaling ‚Üí multiplied by 100 ‚Üí converted to integers.</li>
                <li><strong>BNB</strong>: Binarized data with thresholding (value &gt; 0 = 1, else 0).</li>
                <li><strong>CNB</strong>: Used KBinsDiscretizer to convert features into 10 categorical bins.</li>
            </ul>

            <div class="image-container">
                <img src="assets/nb_data.jpg" alt="Sample of prepared Naive Bayes dataset" class="topic-image">
            </div>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_BNB_X_test.csv" target="_blank" class="github-button">Naive Bayes Datasets</a>
            </div>

            <h2>Model Codebase</h2>
            <p class="fade-in">
                Each Na√Øve Bayes model in this project was implemented through a dedicated Python script. These scripts are tailored to the assumptions and requirements of each model type‚ÄîGaussian, Multinomial, Bernoulli, and Categorical. All models build upon a unified preprocessing step to ensure clean, well-structured data. Below is a breakdown of each modeling component and its role in this project.
            </p>
            
            <!-- Data Preparation -->
            <h3 class="fade-in">üì¶ Data Preparation Script</h3>
            <p class="fade-in">
                The preprocessing workflow loads the cleaned USDA dataset and prepares data in four specific formats. It filters out underrepresented categories, applies a stratified 70-30 train-test split, and tailors the features to match each NB variant:
            </p>
            <ul class="fade-in">
                <li>Continuous features for Gaussian NB</li>
                <li>Scaled, count-like integers for Multinomial NB</li>
                <li>Binary presence/absence indicators for Bernoulli NB</li>
                <li>Discretized categorical bins for Categorical NB</li>
            </ul>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/nb_data_prep.py" target="_blank" class="github-button">View Data Preparation Script</a>
            </div>
            
            <!-- Gaussian Naive Bayes -->
            <h3 class="fade-in">üìä Gaussian Na√Øve Bayes (GNB)</h3>
            <p class="fade-in">
                This model was trained using raw continuous features like calories, fat, protein, and carbohydrate ratios. The goal was to assess how well GNB could classify foods when the input distribution is assumed to be Gaussian. The model demonstrated strong separation in well-behaved, unimodal classes (like "Soda"), but struggled when class boundaries overlapped or distributions were skewed, as seen in categories like "Pizza" and "Frozen Patties."
            </p>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/nb_gnb.py" target="_blank" class="github-button">View GNB Model Script</a>
            </div>
            
            <!-- Multinomial Naive Bayes -->
            <h3 class="fade-in">üìà Multinomial Na√Øve Bayes (MNB)</h3>
            <p class="fade-in">
                This implementation used MinMax-scaled nutritional data converted to non-negative integer-like counts. It was ideal for evaluating frequency-based learning and performed impressively on categories with distinct nutrient patterns. The model achieved high accuracy on items like ‚ÄúPizza‚Äù and ‚ÄúCandy,‚Äù demonstrating that MNB is well-suited for structured and evenly distributed nutritional data.
            </p>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/nb_mnb.py" target="_blank" class="github-button">View MNB Model Script</a>
            </div>
            
            <!-- Bernoulli Naive Bayes -->
            <h3 class="fade-in">‚ö™ Bernoulli Na√Øve Bayes (BNB)</h3>
            <p class="fade-in">
                This model converted all nutritional values into binary flags‚Äîindicating whether a nutrient is present or absent. BNB proved effective in highlighting extreme categories, especially those characterized by nutrient presence (like ‚ÄúCandy‚Äù or ‚ÄúCookies‚Äù). However, it oversimplified the dataset, which led to misclassifications in more nuanced categories due to lost feature granularity.
            </p>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/bnb_model.py" target="_blank" class="github-button">View BNB Model Script</a>
            </div>
            
            <!-- Categorical Naive Bayes -->
            <h3 class="fade-in">üß© Categorical Na√Øve Bayes (CNB)</h3>
            <p class="fade-in">
                CNB used discretized categorical bins (via KBinsDiscretizer) to group continuous values into 10 distinct ranges. While this model is ideal for unordered nominal features, its performance was limited on our structured dataset. Many categories collapsed into a single prediction (e.g., excessive classification as "Soda") due to binning distortion. Nevertheless, this model offered insight into how coarse quantization impacts classification.
            </p>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/nb_cnb.py" target="_blank" class="github-button">View CNB Model Script</a>
            </div>

            <h2>Confusion Matrix Analysis</h2>
            <h3>Gaussian Na√Øve Bayes (GNB)</h3>
            <div class="image-container">
                <img src="visuals/GNB_confusion_matrix_top10_rich.png" alt="GNB Confusion Matrix" class="topic-image">
            </div>
            <p class="fade-in">
                The GNB classifier struggled with significant class overlap, largely due to its core assumption that features are normally (Gaussian) distributed and independent given the class label. While this assumption holds well in some domains, it falls short in complex food datasets where nutrients like sugar, fat, and protein often have correlated patterns. As observed in the confusion matrix, only a few categories such as <strong>‚ÄúSoda‚Äù</strong>, <strong>‚ÄúCookies & Biscuits‚Äù</strong>, and <strong>‚ÄúBreads & Buns‚Äù</strong> achieved near-perfect classification. These categories likely exhibit distinct nutritional signatures (e.g., very high sugar for soda, consistent carb profiles for baked goods), which align well with the Gaussian assumption. 
            
                However, other categories such as <strong>‚ÄúFrozen Patties and Burgers‚Äù</strong> and <strong>‚ÄúPizza‚Äù</strong> were frequently misclassified. These items contain overlapping distributions of fat, protein, and sodium, which violate the independence and normality assumptions, leading to noisy decision boundaries. As a result, GNB is prone to assigning these classes to incorrect categories with similar nutrient profiles. This demonstrates that while GNB offers simplicity and fast computation, it may not generalize well for real-world nutrition data where relationships between nutrients are complex and intertwined.
            </p>

            <h3>Multinomial Na√Øve Bayes (MNB)</h3>
            <div class="image-container">
                <img src="visuals/MNB_confusion_matrix_top10_rich.png" alt="MNB Confusion Matrix" class="topic-image">
            </div>
            <p class="fade-in">
                The Multinomial Na√Øve Bayes (MNB) classifier demonstrated strong performance across multiple food categories, particularly excelling in <strong>‚ÄúPizza‚Äù</strong> with a remarkable <strong>92% classification accuracy</strong>. This suggests that pizza products in the dataset possess a distinct nutritional fingerprint‚Äîlikely a unique combination of high fat, sodium, and carbohydrate values‚Äîthat separates them clearly from other food groups when modeled using frequency-like nutritional features. Similarly, <strong>‚ÄúCandy‚Äù</strong> showed a solid classification accuracy of <strong>68%</strong>, reflecting its identifiable sugar-heavy profile.
            
                However, the model faced challenges differentiating between <strong>‚ÄúCookies & Biscuits‚Äù</strong> and <strong>‚ÄúIce Cream & Frozen Yogurt‚Äù</strong>. Both of these categories share overlapping nutritional patterns, such as high sugar and moderate fat content, leading to significant misclassifications. Despite this, MNB was the <strong>best-performing classifier overall</strong>, benefiting from the count-like nature of our scaled and discretized feature set. It effectively captured the relative frequencies and intensity of nutritional attributes, which made it particularly suitable for balanced categorical distinctions in this dataset. Its performance reinforces MNB‚Äôs suitability for structured nutritional analysis, especially when features are transformed into non-negative integers resembling term frequency distributions.
            </p>

            <h3>Bernoulli Na√Øve Bayes (BNB)</h3>
            <div class="image-container">
                <img src="visuals/BNB_confusion_matrix_top10.png" alt="BNB Confusion Matrix" class="topic-image">
            </div>
            <p class="fade-in">
                The Bernoulli Na√Øve Bayes (BNB) model yielded notably high classification accuracy for categories such as <strong>‚ÄúCookies & Biscuits‚Äù</strong> and <strong>‚ÄúCandy‚Äù</strong>, each achieving over <strong>88%</strong> accuracy. This success suggests that these food groups have a clear binary signature‚Äîi.e., distinct patterns in whether certain nutrients are present or absent above the binarization threshold (e.g., presence of sugar or fat).
            
                However, the model‚Äôs performance was less impressive across other categories. A recurring issue was its tendency to <strong>overclassify</strong> diverse food items as ‚ÄúCookies & Biscuits,‚Äù indicating that many food types shared similar binary profiles, which BNB could not distinguish due to its simplistic input format. By converting continuous nutritional values into binary indicators (present or not), <strong>much of the granularity and variability within the data was lost</strong>. For example, foods with slightly different calorie or fat levels might be treated identically if both values were simply flagged as ‚Äúpresent.‚Äù 
            
                This lack of nuance led to <strong>blurred boundaries</strong> between nutritionally similar categories. While BNB can be effective for high-dimensional binary classification, it <strong>underperforms when the dataset requires understanding subtle differences</strong> in nutrient magnitudes‚Äîsomething continuous or count-based models like GNB or MNB handle better. Overall, BNB delivered interpretable yet limited results, highlighting the trade-off between simplicity and accuracy in binary modeling.
            </p>

            <h3>Categorical Na√Øve Bayes (CNB)</h3>
            <div class="image-container">
                <img src="visuals/CNB_confusion_matrix_top10.png" alt="CNB Confusion Matrix" class="topic-image">
            </div>
            <p class="fade-in">
                The Categorical Na√Øve Bayes (CNB) model struggled significantly in this task, with <strong>over 90% of samples from diverse categories being misclassified as ‚ÄúSoda.‚Äù</strong> This extreme mislabeling highlights a critical flaw in applying CNB to this dataset: the underlying data, which consists of continuous nutritional values, is not inherently categorical. To make CNB compatible, the features had to be discretized using <code>KBinsDiscretizer</code>, converting continuous values (e.g., protein grams or calories) into fixed categorical bins.
            
                While this binning allowed for CNB to process the data, it also introduced a considerable <strong>loss of distributional integrity</strong>. Small differences between items were erased when grouped into the same bin, and crucial statistical variability was replaced by broad categorical labels. This likely led to high intra-bin similarity across distinct food categories, making it difficult for the model to distinguish them meaningfully.
            
                The model appeared to latch onto a dominant bin signature corresponding to ‚ÄúSoda‚Äù and consequently predicted that label across most categories. This behavior underscores how <strong>categorical binning, when applied to non-categorical data, can create artificial similarities</strong> that mislead the learning algorithm. CNB is best reserved for naturally categorical inputs, such as discrete choices (e.g., flavor type, packaging style), rather than numerical nutrition data. In this context, CNB‚Äôs poor performance reinforces the importance of choosing models that align with the true nature of the features.
            </p>

            <h2>Conclusions & Takeaways</h2>
            <ul class="fade-in">
                <li>
                    <strong>Multinomial Na√Øve Bayes (MNB) delivered the most robust performance</strong> across the board. This model‚Äôs effectiveness stems from its ability to model feature frequencies or counts‚Äîclosely resembling the structure of scaled nutritional values like grams of protein, sugar, or fat. Even after converting values to non-negative integers, MNB retained enough numerical resolution to differentiate between subtle food category distinctions. Its classification accuracy for diverse categories such as "Pizza" and "Candy" suggests MNB adapts well to structured, quantity-driven food profiles, making it highly appropriate for datasets based on nutrient-based features.
                </li>
            
                <li>
                    <strong>Bernoulli Na√Øve Bayes (BNB) performed moderately well but oversimplified the dataset.</strong> BNB is ideal for binary classification tasks‚Äîsuch as spam detection or presence/absence indicators‚Äîbut applying it to continuous nutrition data required aggressive binarization. This transformation led to a significant loss of detail. Despite some strong predictions (notably for high-signal categories like ‚ÄúCandy‚Äù), the model frequently misclassified items as "Cookies & Biscuits" due to dominant 1/0 patterns across features. While useful for highlighting presence-based characteristics, BNB lacks the nuance to capture gradual nutritional differences.
                </li>
            
                <li>
                    <strong>Gaussian Na√Øve Bayes (GNB) offered mixed results.</strong> While it operates under the assumption that features are normally distributed, most real-world food data does not adhere to a single-mode bell curve. This mismatch was evident in the model‚Äôs confusion matrices‚Äîonly categories with relatively tight distributions like ‚ÄúSoda‚Äù and ‚ÄúBreads & Buns‚Äù were consistently classified well. More complex or overlapping categories (e.g., ‚ÄúPizza,‚Äù ‚ÄúFrozen Patties‚Äù) led to heavy confusion. GNB might be viable in datasets where features have clean Gaussian distributions, but for diverse food classes, it lacks adaptability.
                </li>
            
                <li>
                    <strong>Categorical Na√Øve Bayes (CNB) performed the weakest overall</strong> due to its fundamental incompatibility with continuous data. The process of binning numerical features into discrete categories using <code>KBinsDiscretizer</code> severely degraded the information content, compressing subtle distinctions into broad buckets. As a result, CNB misclassified over 90% of food items as ‚ÄúSoda,‚Äù revealing how categorical encodings can unintentionally create artificial similarities. CNB is best reserved for datasets where features are naturally nominal‚Äîsuch as flavor type, brand name, or food category‚Äînot scaled nutrient quantities.
                </li>
            </ul>

            <p class="fade-in">
                Overall, Na√Øve Bayes proved efficient and scalable. Despite the feature independence assumption, performance was solid. For real-world food classification, MNB is recommended for count/frequency data, and GNB where distributions align with Gaussian assumptions.
            </p>
        </div>
    </div>




            <!-- Your existing HTML content -->

            <script>
                document.addEventListener("DOMContentLoaded", function() {
                    let text = "Naive Bayes";
                    let i = 0;
                    function typeWriter() {
                        if (i < text.length) {
                            document.getElementById("typing-title").innerHTML += text.charAt(i);
                            i++;
                            setTimeout(typeWriter, 50);
                        }
                    }
                    typeWriter();
                });
            </script>

        <!-- üîπ JavaScript for Scroll-Based Fade-In Effect -->
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                let fadeInElements = document.querySelectorAll(".fade-in");
    
                function fadeInOnScroll() {
                    fadeInElements.forEach(element => {
                        let position = element.getBoundingClientRect().top;
                        let screenHeight = window.innerHeight;
    
                        if (position < screenHeight - 100) {
                            element.classList.add("visible");
                        }
                    });
                }
    
                window.addEventListener("scroll", fadeInOnScroll);
                fadeInOnScroll(); // Trigger on load
            });
        </script>

    <script>
        function toggleMenu() {
            var navbar = document.querySelector(".navbar");
            navbar.classList.toggle("active");
        }
    </script>
    <script type="module" src="animations.js"></script>
    <div class="transition-overlay"></div>
    <script type="module" src="animations.js"></script>
</body>
</html>