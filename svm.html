<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SVM</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">

</head>
<body>

      <!-- Elegant Animated Background -->
      <div class="food-background"></div>
      <div class="light-overlay"></div>

    <!-- üîπ Hamburger Menu Icon -->
    <div class="menu-icon" onclick="toggleMenu()">‚ò∞</div>
    <div class="navbar">
        <a href="index.html">Home</a>
        <a href="introduction.html">Introduction</a>
        <a href="data_prep.html">Data Prep</a>
        <a href="eda.html">EDA</a>        
        <a href="pca.html">PCA</a>
        <a href="clustering.html">Clustering</a>
        <a href="arm.html">ARM</a>
        <a href="decision_trees.html">Decision Trees</a>
        <a href="naive_bayes.html">Naive Bayes</a>
        <a href="svm.html"class="active">SVM</a>
        <a href="regression.html">Regression</a>
        <a href="randomforest.html">Random Forest</a>
        <a href="conclusions.html">Conclusions</a>
        <a href="about_me.html">About Me</a>
    </div>

    <div class="content-section">
        <h1 id="typing-title"></h1>
        <div class="content-wrapper">
            <!-- üìö Introduction to Support Vector Machines -->
            <h2>What Are Support Vector Machines (SVMs)?</h2>
            <p class="fade-in">
                <strong>Support Vector Machines (SVMs)</strong> are among the most robust and versatile supervised learning models, widely applied in both classification and regression scenarios. 
                Their central objective is to identify the <strong>optimal hyperplane</strong>‚Äîa boundary that not only separates classes but does so with the maximum possible margin between the closest data points of each class.
                This margin maximization principle is fundamental, as it enhances the model‚Äôs ability to generalize to unseen data and resist overfitting.
                In two-dimensional space, this optimal hyperplane is a straight line; in higher dimensions, it extends to planes or hyperplanes.
                The beauty of SVMs lies in how they rely on a handful of pivotal data points‚Äîcalled <strong>support vectors</strong>‚Äîto determine this decision boundary.
                These vectors lie precisely at the margin‚Äôs edge and hold all the information needed to define the hyperplane.
                By focusing only on these key examples, SVMs create a decision boundary that is both mathematically elegant and computationally efficient, making them especially valuable in high-dimensional feature spaces.
            </p>
        
            <div class="gif-section">
                <img src="assets/svm.gif" alt="SVM Process Animation" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>
        
            <p class="fade-in">
                However, real-world data is often <strong>not linearly separable</strong>. To handle such cases, SVMs use <strong>kernel functions</strong> to transform data into higher dimensions where it may become linearly separable.
                This technique is called the <strong>kernel trick</strong>. A kernel function implicitly computes the dot product in the transformed feature space, without ever computing the transformation explicitly.
                This not only increases computational efficiency but also unlocks the ability to learn complex, non-linear patterns.
            </p>
        
            <h2>How Do Kernels Work?</h2>
            <p class="fade-in">
                The kernel plays a pivotal role in SVM modeling, serving as a bridge to project data into higher-dimensional spaces where complex relationships become easier to separate. 
                Instead of explicitly computing this transformation, SVMs rely on the <strong>kernel trick</strong>, which calculates the dot product of the data points in the transformed space directly‚Äîsaving immense computational effort.
                For instance, a <strong>polynomial kernel</strong> of degree 2 can take two-dimensional input data and implicitly map it into a three-dimensional or higher space, allowing the model to draw curved boundaries in the original plane that could not have been achieved otherwise.
                This kernel is defined by the formula: <code>K(x, y) = (x ‚ãÖ y + r)<sup>d</sup></code>, where <code>r</code> is a free constant (commonly set to 1) and <code>d</code> is the degree of the polynomial expansion.
                Another widely used choice is the <strong>Radial Basis Function (RBF)</strong> kernel, expressed as 
                <code>K(x, y) = exp(-Œ≥||x - y||<sup>2</sup>)</code>, which transforms data by creating smooth, flexible decision boundaries.
                The RBF kernel is especially powerful when classes are not linearly separable even after polynomial transformations, enabling SVMs to handle intricate and non-linear classification problems with remarkable precision.
            </p>
        
            <div class="image-container">
                <img src="assets/svm_kernel_example.png" alt="SVM Kernel Transformation" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>

            <p class="fade-in">
                The image above perfectly captures the essence of the <strong>kernel trick</strong> in SVMs. On the left, we see a dataset in its original two-dimensional space, where the two classes (represented as red squares and green circles) are clearly not linearly separable‚Äîno straight line can divide them without error. This is a common scenario in real-world datasets.
            </p>
            
            <p class="fade-in">
                By applying a kernel function, such as the <strong>Radial Basis Function (RBF)</strong> or a <strong>polynomial kernel</strong>, we project the data into a higher-dimensional space (illustrated on the right). Here, the separation becomes feasible: the red class has been elevated into a different spatial region than the green class. In this transformed space, a simple flat plane‚Äîcalled a <strong>decision surface</strong>‚Äîcan now cleanly divide the two categories.
            </p>
            
            <p class="fade-in">
                This transformation is the power of SVM kernels: they allow the model to learn <strong>non-linear decision boundaries</strong> without ever explicitly computing the new dimensions. The surface that separates the classes is learned implicitly using the kernel's dot product calculations. This enables SVMs to perform exceptionally well even when the data is entangled in complex, curved, or spiral-shaped structures in its original form.
            </p>
        
            <h2>üîç Casting to Higher Dimensions ‚Äì A Concrete Example</h2>
            <p class="fade-in">
                Suppose we have two points in 2D space: <code>a = (a‚ÇÅ, a‚ÇÇ)</code> and <code>b = (b‚ÇÅ, b‚ÇÇ)</code>. A polynomial kernel with <code>r = 1</code> and <code>d = 2</code> transforms the dot product as follows:
            </p>
            <pre class="fade-in"><code>(a ‚ãÖ b + 1)¬≤ = (a‚ÇÅb‚ÇÅ + a‚ÇÇb‚ÇÇ + 1)¬≤</code></pre>
            <p class="fade-in">
                This expands algebraically to: 
                <code>a‚ÇÅ¬≤b‚ÇÅ¬≤ + 2a‚ÇÅa‚ÇÇb‚ÇÅb‚ÇÇ + a‚ÇÇ¬≤b‚ÇÇ¬≤ + 2a‚ÇÅb‚ÇÅ + 2a‚ÇÇb‚ÇÇ + 1</code>,
                revealing that a simple dot product between transformed vectors results in terms that include not only the original features but also their squared values and cross-products. 
                In practical terms, this means that each original 2D input vector is effectively cast into a <strong>six-dimensional feature space</strong>, incorporating linear terms (<code>a‚ÇÅb‚ÇÅ</code>, <code>a‚ÇÇb‚ÇÇ</code>), interaction terms (<code>a‚ÇÅa‚ÇÇb‚ÇÅb‚ÇÇ</code>), and second-order polynomials (<code>a‚ÇÅ¬≤b‚ÇÅ¬≤</code>, <code>a‚ÇÇ¬≤b‚ÇÇ¬≤</code>). 
                This transformation allows the model to draw nonlinear boundaries in the original space by learning linear separators in this enriched space. 
                The elegance of the kernel trick lies in the fact that these expanded features are never manually computed or stored. 
                Instead, the SVM operates entirely through kernel evaluations, maintaining efficiency while reaping the benefits of working in higher dimensions. 
                This approach enables SVMs to model intricate decision surfaces that adapt to complex data patterns, all without compromising computational performance.
            </p>
        
            <p class="fade-in">
                In essence, kernel functions empower Support Vector Machines (SVMs) to handle complex, non-linearly separable data by implicitly mapping it into higher-dimensional spaces where linear separation becomes feasible. This technique‚Äîknown as the "kernel trick"‚Äîeliminates the computational burden of actually transforming the data, allowing SVMs to operate efficiently even in infinite-dimensional feature spaces. Whether the data is arranged in concentric circles, intricate spirals, or tangled clusters, kernels such as the radial basis function (RBF) or polynomial kernel provide the mathematical flexibility to define nuanced decision boundaries. By focusing on dot products in transformed space, kernels let SVMs identify optimal margins that would be impossible in the original feature space. This mechanism unlocks the true power of SVMs, extending their applicability far beyond simple problems and enabling robust performance in real-world domains like image recognition, bioinformatics, and nutritional data classification. In short, kernels make SVMs a formidable tool by drawing complex boundaries‚Äîwithout explicitly computing complex transformations.
            </p>

            <!-- SVM Data Preparation -->
            <h2>Data Preparation for Support Vector Machines (SVM)</h2>
            <p class="fade-in">
                Support Vector Machines are supervised learning models that rely on <strong>labeled, numeric data</strong>. 
                For our SVM analysis, we began with the <code>clean_normalized_data.csv</code> file. This file contains nutritional attributes such as 
                <strong>calories, protein, fat, carbohydrates, and derived ratios</strong> for each food item. These continuous variables were already normalized,
                making the dataset ideal for distance-based algorithms like SVMs.
            </p>

            <div class="image-container">
                <img src="assets/clean_normalized_data.png" alt="Clean Normalized Data Preview" class="topic-image" style="max-width: 100%; border-radius: 8px;">
            </div>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/clean_normalized_data.csv" target="_blank" class="github-button">View Dataset</a>
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/svm_binary_data.py" target="_blank" class="github-button">View Preprocessing Script</a>
            </div>

            <h3>Selected Categories for Binary Classification</h3>
            <p class="fade-in">
                For this SVM modeling task, we focused on two nutritionally dense and clearly distinguishable ultra-processed food categories: 
                <strong>Potato Chips</strong> and <strong>Cookies and Brownies</strong>. 
                These categories were selected due to their distinct nutrient profiles‚Äî<em>Potato Chips</em> are typically high in sodium and fats, 
                while <em>Cookies and Brownies</em> tend to be rich in sugars and carbohydrates. The nutritional divergence between these classes 
                makes them an excellent candidate pair for binary classification using SVM, where a clear margin between the classes can be learned effectively.
            </p>
            
            <h3>Train-Test Splitting and Feature Handling</h3>
            <p class="fade-in">
                After filtering the dataset to include only the two selected categories, we isolated the numeric features relevant to modeling, 
                such as <strong>calories, protein, fat, carbohydrates</strong>, and engineered ratios like <code>calories_per_fat</code> and <code>calories_per_protein</code>. 
                Non-numeric columns like food <code>description</code>, <code>category</code>, and <code>brand</code> were intentionally removed 
                to comply with the input requirements of SVMs, which operate purely on numeric vectors in multi-dimensional space.
            </p>
            
            <p class="fade-in">
                To maintain the proportional representation of each class, we applied a <strong>stratified 70-30 split</strong> using 
                <code>train_test_split(..., stratify=y)</code>. This ensures that both the training and testing datasets contain an equal distribution 
                of samples from each class. Stratification is critical in binary classification problems with imbalanced data, as it prevents the model from 
                being biased toward the majority class.
            </p>
            
            <p class="fade-in">
                Importantly, the training and testing sets are <strong>disjoint</strong>, meaning there is zero overlap in rows between the two sets. 
                This is essential in supervised machine learning to avoid <em>data leakage</em>‚Äîa condition where the model inadvertently gains access 
                to information from the test set during training, leading to unrealistically high accuracy. By ensuring that the test set remains unseen 
                during the training process, we allow for an authentic evaluation of how well the model can generalize to new, unseen data.
            </p>
            
            <p class="fade-in">
                Prior to modeling, the features were passed through a <strong>MinMaxScaler</strong> to rescale all values to a uniform [0,1] range. 
                This normalization step ensures that features with larger numerical ranges (e.g., calorie count) do not dominate those with smaller 
                ranges (e.g., protein ratios) during the distance calculations used in kernel functions. Without this step, the SVM‚Äôs decision boundary 
                could be skewed, resulting in suboptimal performance.
            </p>

            <!-- X Train -->
            <h4 class="fade-in">üìÅ SVM_X_Train: Feature matrix used for training</h4>
            <div class="image-container">
                <img src="assets/svm_X_train.jpg" alt="SVM X Train Dataset" class="topic-image" style="max-width: 100%; border-radius: 8px;">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/svm_X_train.csv" target="_blank" class="github-button">View SVM_X_Train.csv</a>
            </div>

            <!-- Y Train -->
            <h4 class="fade-in">üè∑Ô∏è SVM_Y_Train: Target labels for training set</h4>
            <div class="image-container">
                <img src="assets/svm_Y_train.jpg" alt="SVM Y Train Labels" class="topic-image" style="max-width: 100%; border-radius: 8px;">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/svm_y_train.csv" target="_blank" class="github-button">View SVM_Y_Train.csv</a>
            </div>

            <!-- X Test -->
            <h4 class="fade-in">üìÅ SVM_X_Test: Feature matrix used for evaluation</h4>
            <div class="image-container">
                <img src="assets/svm_X_test.jpg" alt="SVM X Test Dataset" class="topic-image" style="max-width: 100%; border-radius: 8px;">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/svm_X_test.csv" target="_blank" class="github-button">View SVM_X_Test.csv</a>
            </div>

            <!-- Y Test -->
            <h4 class="fade-in">üè∑Ô∏è SVM_Y_Test: True labels used to validate performance</h4>
            <div class="image-container">
                <img src="assets/svm_Y_test.jpg" alt="SVM Y Test Labels" class="topic-image" style="max-width: 100%; border-radius: 8px;">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/svm_y_test.csv" target="_blank" class="github-button">View SVM_Y_Test.csv</a>
            </div>


            <p class="fade-in">
                These disjoint datasets ensure that model evaluation is performed on unseen data, preventing <strong>data leakage</strong> and preserving generalization.
                This clean split is essential for fair benchmarking and for comparing the performance of different SVM kernels (Linear, Polynomial, RBF) in subsequent steps.
            </p>

            <!-- SVM Modeling and Evaluation -->
            <h2>Support Vector Machine (SVM) Modeling</h2>

            <p class="fade-in">
                To assess how different SVM kernels perform on ultra-processed food classification, we ran extensive modeling and evaluation using
                <code>svm_kernel_compare.py</code>. This script iteratively trains SVM models using <strong>three different kernel types</strong>‚ÄîLinear, Polynomial, and RBF‚Äîeach across three cost values: <code>C=0.1</code>, <code>C=1</code>, and <code>C=10</code>.
            </p>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/svm_kernel_compare.py" target="_blank" class="github-button">View SVM Modeling Code</a>
            </div>

            <h3>Results and Confusion Matrices</h3>

            <p class="fade-in">
                For each configuration, we recorded the <strong>confusion matrix</strong>, <strong>classification report</strong>, and <strong>accuracy score</strong>. These results were saved in the <code>/visuals</code> and <code>/reports</code> directories respectively.
            </p>
            
            <!-- Linear Kernels -->
            <h4 class="fade-in">üîπ Linear Kernel</h4>

            <!-- Linear C=0.1 -->
            <div class="image-container">
                <img src="visuals/svm_conf_matrix_linear_C0.1.png" alt="Linear Kernel C=0.1 Confusion Matrix" class="topic-image">
            </div>
            <p class="fade-in">
                With <strong>C=0.1</strong>, the linear kernel model exhibits high bias and fails to capture the complexity of the class boundaries. 
                It classifies all instances as Class 0 (majority class), completely misclassifying every Class 1 sample. 
                The precision for Class 0 is 79%, but recall for Class 1 is zero, leading to a substantial degradation in overall model reliability for minority detection. 
                Although the reported accuracy stands at <strong>79.49%</strong>, this value is misleading because it reflects the imbalance favoring Class 0.
            </p>

            <div class="image-container">
                <img src="assets/svm_linear_C0.1_report.jpg" alt="SVM Linear C=0.1 Classification Report" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/reports/svm_linear_C0.1_report.txt" target="_blank" class="github-button">View Linear C=0.1 Report</a>
            </div>

            <!-- Linear C=1 -->
            <div class="image-container">
                <img src="visuals/svm_conf_matrix_linear_C1.png" alt="Linear Kernel C=1 Confusion Matrix" class="topic-image">
            </div>
            <p class="fade-in">
                At <strong>C=1</strong>, the linear kernel model improves significantly. 
                It correctly identifies half of the Class 1 samples, while maintaining strong performance on Class 0 with 88% precision and 97% recall.
                The overall model accuracy rises to <strong>87.18%</strong>. 
                This configuration strikes a better balance between bias and variance, allowing the model to generalize more effectively without overfitting.
                The confusion matrix shows fewer false negatives, resulting in greater predictive reliability across both classes.
            </p>

            <div class="image-container">
                <img src="assets/svm_linear_C1_report.jpg" alt="SVM Linear C=1 Classification Report" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/reports/svm_linear_C1_report.txt" target="_blank" class="github-button">View Linear C=1 Report</a>
            </div>

            <!-- Linear C=10 -->
            <div class="image-container">
                <img src="visuals/svm_conf_matrix_linear_C10.png" alt="Linear Kernel C=10 Confusion Matrix" class="topic-image">
            </div>
            <p class="fade-in">
                Increasing the cost parameter to <strong>C=10</strong> does not yield any improvement over the C=1 model. 
                The confusion matrix and classification metrics remain identical, with an accuracy of <strong>87.18%</strong>.
                This suggests that the linear kernel has already captured the best linear boundary possible for this data, and additional penalization of misclassification does not improve performance.
                Further increases in C could lead to overfitting, but without visible performance gains, highlighting the limits of linear separation for this dataset.
            </p>

            <div class="image-container">
                <img src="assets/svm_linear_C10_report.jpg" alt="SVM Linear C=10 Classification Report" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/reports/svm_linear_C10_report.txt" target="_blank" class="github-button">View Linear C=10 Report</a>
            </div>


            <!-- Polynomial Kernels -->
            <h4 class="fade-in">üî∏ Polynomial Kernel</h4>

            <!-- Polynomial C=0.1 -->
            <div class="image-container">
                <img src="visuals/svm_conf_matrix_poly_C0.1.png" alt="Polynomial Kernel C=0.1" class="topic-image">
            </div>
            <div class="image-container">
                <img src="assets/svm_poly_C0.1_report.jpg" alt="Polynomial Kernel Report C=0.1" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>
            <p class="fade-in">
                With <strong>C=0.1</strong>, the Polynomial Kernel achieved an accuracy of <strong>87.18%</strong>. 
                Class 0 was predicted with perfect recall (1.00), meaning all instances of Class 0 were correctly classified. 
                However, the model struggled to fully capture Class 1, with a recall of only 0.38. 
                While the precision for Class 1 was perfect (1.00), indicating that when it predicted Class 1 it was correct, it still missed over half of the true Class 1 examples.
                This highlights a tendency of the model to be conservative with minority class predictions at lower C values.
            </p>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/reports/svm_poly_C0.1_report.txt" target="_blank" class="github-button">View Poly C=0.1 Report</a>
            </div>

            <!-- Polynomial C=1 -->
            <div class="image-container">
                <img src="visuals/svm_conf_matrix_poly_C1.png" alt="Polynomial Kernel C=1" class="topic-image">
            </div>
            <div class="image-container">
                <img src="assets/svm_poly_C1_report.jpg" alt="Polynomial Kernel Report C=1" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>
            <p class="fade-in">
                Increasing the cost parameter to <strong>C=1</strong> resulted in no change to the confusion matrix or classification metrics‚Äîthe accuracy remained at <strong>87.18%</strong>. 
                The model retained perfect precision for Class 1 but continued to suffer from low recall (0.38) for that class. 
                This suggests that simply increasing the penalty on misclassification (C) was not enough to shift the decision boundary to favor better detection of minority samples with this kernel.
            </p>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/reports/svm_poly_C1_report.txt" target="_blank" class="github-button">View Poly C=1 Report</a>
            </div>

            <!-- Polynomial C=10 -->
            <div class="image-container">
                <img src="visuals/svm_conf_matrix_poly_C10.png" alt="Polynomial Kernel C=10" class="topic-image">
            </div>
            <div class="image-container">
                <img src="assets/svm_poly_C10_report.jpg" alt="Polynomial Kernel Report C=10" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>
            <p class="fade-in">
                At a higher cost value of <strong>C=10</strong>, the classification results remained identical to C=0.1 and C=1. 
                No improvement in recall, precision, or overall accuracy was observed. 
                This stability indicates that for this dataset, the polynomial kernel's flexibility was not sensitive to regularization strength, 
                and further hyperparameter tuning (like changing the polynomial degree) would likely be needed to significantly improve minority class capture.
            </p>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/reports/svm_poly_C10_report.txt" target="_blank" class="github-button">View Poly C=10 Report</a>
            </div>
            
            <!-- RBF Kernels -->
            <h4 class="fade-in">üîπ RBF Kernel</h4>

            <p class="fade-in">
                The <strong>Radial Basis Function (RBF)</strong> kernel, also known as the Gaussian kernel, is one of the most powerful tools for non-linear classification tasks. 
                By mapping input features into an infinite-dimensional space, it enables linear separation of data that is otherwise inseparable in the original feature space. 
                Below are detailed results at different values of <code>C</code> (the regularization strength), illustrating how tuning this parameter affects model complexity, bias, and generalization.
            </p>

            <!-- C = 0.1 -->
            <div class="image-container">
                <img src="visuals/svm_conf_matrix_rbf_C0.1.png" alt="RBF Kernel C=0.1" class="topic-image">
            </div>
            <p class="fade-in">
                At <strong>C=0.1</strong>, the RBF model heavily prioritized margin maximization, allowing more misclassifications to create a wider margin. 
                While Class 0 (Potato Chips) was perfectly classified with a recall of 100%, the model completely failed to capture Class 1 (Cookies & Brownies)‚Äîresulting in zero recall and precision for that class. 
                The overall accuracy remained deceptively high at <strong>79.49%</strong> due to class imbalance but the macro-averaged F1 score was poor, highlighting serious shortcomings in minority class detection.
            </p>
            <div class="image-container">
                <img src="assets/svm_rbf_C0.1_report.jpg" alt="RBF Kernel C=0.1 Report Snapshot" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/reports/svm_rbf_C0.1_report.txt" target="_blank" class="github-button">View RBF C=0.1 Report</a>
            </div>

            <!-- C = 1 -->
            <div class="image-container">
                <img src="visuals/svm_conf_matrix_rbf_C1.png" alt="RBF Kernel C=1" class="topic-image">
            </div>
            <p class="fade-in">
                Upon increasing the regularization to <strong>C=1</strong>, the model's performance improved dramatically. 
                The accuracy rose to an impressive <strong>89.74%</strong>. 
                Class 0 was perfectly captured with high precision and recall, and 50% of Class 1 instances were correctly identified. 
                Macro and weighted averages across precision, recall, and F1-score also reflected balanced generalization, making this setting the best among RBF models tested.
            </p>
            <div class="image-container">
                <img src="assets/svm_rbf_C1_report.jpg" alt="RBF Kernel C=1 Report Snapshot" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/reports/svm_rbf_C1_report.txt" target="_blank" class="github-button">View RBF C=1 Report</a>
            </div>

            <!-- C = 10 -->
            <div class="image-container">
                <img src="visuals/svm_conf_matrix_rbf_C10.png" alt="RBF Kernel C=10" class="topic-image">
            </div>
            <p class="fade-in">
                At <strong>C=10</strong>, despite tighter margins and a stronger penalty on misclassification, the model‚Äôs performance slightly declined. 
                It overfit on the training data, leading to reduced flexibility on unseen test data. 
                Although Class 0 remained well-predicted, Class 1 performance degraded, as seen by the drop in recall and F1-score. 
                The final accuracy dropped to <strong>87.18%</strong>, emphasizing the importance of balanced regularization.
            </p>
            <div class="image-container">
                <img src="assets/svm_rbf_C10_report.jpg" alt="RBF Kernel C=10 Report Snapshot" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/reports/svm_rbf_C10_report.txt" target="_blank" class="github-button">View RBF C=10 Report</a>
            </div>

            <h3>Kernel Performance Comparison</h3>

            <p class="fade-in">
                The bar chart below presents a comprehensive side-by-side comparison of the <strong>accuracy scores</strong> achieved by each SVM kernel‚Äî<code>linear</code>, <code>polynomial</code>, and <code>rbf</code>‚Äîevaluated across three different values of the regularization parameter <strong>C</strong>: 0.1, 1, and 10.
                Each kernel introduces a different method of transforming or interpreting the input feature space: 
                the <strong>linear kernel</strong> fits a straight hyperplane, 
                the <strong>polynomial kernel</strong> introduces higher-degree feature interactions, 
                and the <strong>radial basis function (RBF) kernel</strong> maps inputs into an infinite-dimensional space to capture non-linear patterns.
            </p>
            
            <p class="fade-in">
                The <strong>regularization parameter C</strong> plays a critical role: 
                smaller values (e.g., C=0.1) emphasize a wider margin at the expense of training accuracy (potential underfitting), 
                while larger values (e.g., C=10) push the model to classify training examples more accurately, risking overfitting.
                Observing kernel behavior across these three C values allows us to study how each method balances the bias-variance trade-off.
            </p>
            
            <p class="fade-in">
                From the comparative analysis, it is evident that <strong>increasing C generally improved model performance up to a point</strong>, particularly for the RBF kernel, which achieved the highest overall accuracy at C=1 (approximately 90%). 
                The <strong>linear kernel</strong> demonstrated stability between C=1 and C=10, but struggled to perfectly separate complex patterns in the data.
                Meanwhile, the <strong>polynomial kernel</strong> maintained consistent accuracy across all C values but exhibited slightly less sensitivity to model complexity adjustments.
                Overall, the RBF kernel outperformed others in balancing precision, recall, and generalization, making it the most suitable for this specific two-class food classification task involving <em>Potato Chips</em> and <em>Cookies and Brownies</em>.
            </p>
            
            <div class="image-container">
                <img src="visuals/svm_kernel_accuracy_comparison.png" alt="SVM Kernel Accuracy Comparison" class="topic-image">
            </div>
            
            <p class="fade-in">
                üîπ <strong>Linear Kernel:</strong> Performance noticeably improved as the regularization parameter <code>C</code> increased. 
                With a low <code>C = 0.1</code>, the model suffered from underfitting, predicting only the majority class correctly while entirely failing to recognize the minority class (Class 1).
                As <code>C</code> increased to 1 and 10, the model's ability to distinguish between classes stabilized, achieving an accuracy of <strong>87.18%</strong> in both cases. 
                Despite the better results, the linear kernel struggled to fully capture the complex patterns between the food categories, showing that linear decision boundaries were somewhat limiting for our dataset.
            </p>
            
            <p class="fade-in">
                üî∏ <strong>Polynomial Kernel:</strong> The polynomial kernel demonstrated very stable, albeit modest, performance across all tested values of <code>C</code>. 
                Accuracy consistently hovered around <strong>87.18%</strong>, but the recall for the minority class remained low, indicating a persistent difficulty in identifying Class 1 samples correctly.
                Even as <code>C</code> increased, the added decision boundary complexity did not translate into better generalization. 
                This highlights a key limitation: while polynomial transformations theoretically allow for curved and complex decision boundaries, 
                in practice they can lead to overfitting or flat performance gains if the degree of the polynomial and other hyperparameters are not carefully tuned for the specific data distribution.
            </p>
            
            <p class="fade-in">
                üîπ <strong>RBF Kernel:</strong> The RBF (Radial Basis Function) kernel outshined both the linear and polynomial kernels in this task. 
                At <code>C = 1</code>, it achieved the highest observed accuracy of <strong>89.74%</strong>, effectively balancing flexibility and generalization.
                It successfully captured non-linear separations between the two food categories, distinguishing both majority and minority classes far better than the other kernels.
                Although performance dipped slightly at <code>C = 10</code> due to minor overfitting tendencies, the RBF kernel clearly demonstrated its power in modeling complex, real-world boundaries when minimal hyperparameter tuning is desired.
            </p>
            
            <p class="fade-in">
                üí° <strong>Key Takeaway:</strong> Among all the tested kernels, the <strong>RBF kernel with C=1</strong> emerged as the most reliable performer, consistently balancing sensitivity to minority classes with overall specificity. 
                Its inherent ability to model complex, non-linear relationships made it exceptionally suited for distinguishing food items with subtle and overlapping nutrient profiles. 
                Although the <strong>linear kernel</strong> offered simplicity, speed, and interpretability, it lacked the flexibility needed to capture intricate patterns present in real-world nutritional data. 
                On the other hand, the <strong>polynomial kernel</strong> provided theoretical adaptability through higher-order transformations but added unnecessary noise, 
                ultimately resulting in stagnant accuracy gains and limited practical benefits. 
                Given these observations, the <strong>RBF kernel</strong> stands out as the most appropriate choice for tasks like food classification, nutrition modeling, or any problem where 
                <em>complex class boundaries</em> and <em>high-dimensional feature spaces</em> are expected. 
                It offers the best trade-off between accuracy, generalization, and robustness‚Äîqualities that are essential for building models that can be trusted in critical real-world applications.
            </p>

            <!-- SVM Conclusions -->
            <h2>Conclusions</h2>

            <p class="fade-in">
                This SVM modeling exercise provided several key insights into the classification of ultra-processed food categories‚Äîspecifically distinguishing between <strong>Potato Chips</strong> and <strong>Cookies & Brownies</strong>.
                These categories, although both highly processed, possess distinct nutritional signatures that make them ideal candidates for supervised binary classification.
            </p>

            <p class="fade-in">
                The experiments revealed that <strong>kernel choice</strong> and <strong>cost parameter tuning</strong> significantly impact model performance. Linear kernels, while fast and interpretable, struggled slightly with complex class boundaries‚Äîespecially at lower cost values.
                Polynomial kernels captured non-linear relationships but plateaued in performance and risked overfitting. The standout performer was the <strong>RBF kernel with C = 1</strong>, which struck the best balance between <strong>generalization and accuracy</strong>.
            </p>

            <p class="fade-in">
                From a practical standpoint, this modeling process reinforces the idea that food classification is not always a linear problem. Nutritional dimensions such as saturated fat, sugar content, and carbohydrate ratios often interact in <strong>non-linear ways</strong>.
                This makes kernels like RBF ideal for real-world dietary pattern recognition, health diagnostics, or recommendation systems.
            </p>

            <p class="fade-in">
                More broadly, this analysis demonstrates how <strong>Support Vector Machines</strong> can be leveraged to detect patterns within complex health and nutrition data.
                The ability to model nuanced category differences with relatively little training data speaks to SVM's robustness in <strong>high-dimensional, structured domains</strong> like food science.
            </p>

            <!-- Symbolic Summary Visual -->
            <div class="image-container">
                <img src="assets/svm_ultra_vs_real.jpg" alt="Symbolic Processed vs Whole Food Tray" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>

            <p class="fade-in">
                The image above visually encapsulates the broader issue at hand: the growing divide between whole, nourishing foods and ultra-processed, convenience-driven choices.
                While models like SVM help in classifying these foods, they also shed light on underlying consumption patterns that affect health outcomes at a population level.
            </p>

            <!-- Summary Chart -->
            <div class="image-container">
                <img src="visuals/svm_kernel_accuracy_comparison.png" alt="SVM Accuracy Comparison Summary" class="topic-image" style="max-width: 100%; border-radius: 8px; margin-top: 20px;">
            </div>

            <p class="fade-in">
                As shown above, the accuracy distribution across kernel types further reinforces the RBF kernel‚Äôs superior performance on this task. For future iterations of this project, one could explore multi-class SVMs, add more nuanced food categories, or even integrate text features (like ingredient lists) for a hybrid model.
                In conclusion, SVMs not only classify food effectively‚Äîthey help reveal the hidden structure within what we eat, and offer a data-driven lens through which we can understand the food landscape more critically.
            </p>

            
        </div>
    </div>

        <!-- Your existing HTML content -->

        <script>
            document.addEventListener("DOMContentLoaded", function() {
                let text = "Support Vector Machine (SVM)";
                let i = 0;
                function typeWriter() {
                    if (i < text.length) {
                        document.getElementById("typing-title").innerHTML += text.charAt(i);
                        i++;
                        setTimeout(typeWriter, 50);
                    }
                }
                typeWriter();
            });
        </script>

    <!-- üîπ JavaScript for Scroll-Based Fade-In Effect -->
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            let fadeInElements = document.querySelectorAll(".fade-in");

            function fadeInOnScroll() {
                fadeInElements.forEach(element => {
                    let position = element.getBoundingClientRect().top;
                    let screenHeight = window.innerHeight;

                    if (position < screenHeight - 100) {
                        element.classList.add("visible");
                    }
                });
            }

            window.addEventListener("scroll", fadeInOnScroll);
            fadeInOnScroll(); // Trigger on load
        });
    </script>

    <script>
        function toggleMenu() {
            var navbar = document.querySelector(".navbar");
            navbar.classList.toggle("active");
        }
    </script>
    <script type="module" src="animations.js"></script>
    <div class="transition-overlay"></div>
    <script type="module" src="animations.js"></script>
</body>
</html>