Clustering Method,Concept,Advantages,Limitations,Best Use Cases
K-Means,Partitions data into k clusters by minimizing variance within clusters.,"âœ”ï¸ Efficient for large datasets.
âœ”ï¸ Works well for well-separated clusters.
âœ”ï¸ Computationally fast.","âŒ Assumes spherical clusters.
âŒ Sensitive to outliers.
âŒ Requires specifying k.","ğŸ“Œ When clusters are compact and well-separated.
ğŸ“Œ When a fast and scalable solution is needed."
Hierarchical Clustering,Builds a nested hierarchy of clusters by merging or splitting groups based on distance.,"âœ”ï¸ No need to specify k.
âœ”ï¸ Produces dendrograms for visualization.
âœ”ï¸ Captures hierarchical relationships.","âŒ Computationally expensive for large datasets.
âŒ Can be sensitive to minor data variations.
âŒ Difficult to scale.","ğŸ“Œ When understanding relationships between clusters is important.
ğŸ“Œ Works well for small datasets."
DBSCAN,"Groups points based on density, marking outliers as noise.","âœ”ï¸ Automatically determines clusters.
âœ”ï¸ Detects arbitrary-shaped clusters.
âœ”ï¸ Identifies outliers.","âŒ Requires careful tuning of epsilon & minPts.
âŒ Struggles with varying densities.
âŒ May classify dense noise as clusters.","ğŸ“Œ When outlier detection is needed.
ğŸ“Œ When clusters have irregular shapes.
ğŸ“Œ When the number of clusters is unknown."
