<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Decision Trees</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>

   <!-- Elegant Animated Background -->
   <div class="food-background"></div>
   <div class="light-overlay"></div>

    <!-- üîπ Hamburger Menu Icon -->
    <div class="menu-icon" onclick="toggleMenu()">‚ò∞</div>
    <div class="navbar">
        <a href="index.html">Home</a>
        <a href="introduction.html">Introduction</a>
        <a href="data_prep.html">Data Prep</a>
        <a href="eda.html">EDA</a>        
        <a href="pca.html">PCA</a>
        <a href="clustering.html">Clustering</a>
        <a href="arm.html">ARM</a>
        <a href="decision_trees.html"class="active">Decision Trees</a>
        <a href="naive_bayes.html">Naive Bayes</a>
        <a href="svm.html">SVM</a>
        <a href="regression.html">Regression</a>
        <a href="conclusions.html">Conclusions</a>
        <a href="about_me.html">About Me</a>
    </div>

    <div class="content-section">
        <h1 id="typing-title"></h1>
        <div class="content-wrapper">
            
            <!-- (a) Overview -->
            <h2>Introduction to Decision Trees for Food Classification</h2>
            <p class="fade-in">
                Decision Trees (DTs) are supervised machine learning models used for both classification and regression tasks. These models split the data recursively based on feature values to form a tree-like structure where each internal node represents a decision rule, each branch an outcome, and each leaf node a predicted class label.
            </p>

            <p class="fade-in">
                Decision Trees are widely used due to their interpretability, ability to handle mixed data types, and low data preprocessing requirements. However, they are prone to overfitting and can grow very deep if not pruned. Below are two visual illustrations to help understand their structure and functioning:
            </p>

            <div class="image-container">
                <img src="assets/DT_structure.png" alt="Decision Tree Structure" class="topic-image">
            </div>

            <div class="image-container">
                <img src="assets/infogain.jpg" alt="Information Gain Illustration" class="topic-image">
            </div>

            <p class="fade-in">
                <strong>Impurity Metrics:</strong> Decision Trees evaluate potential splits using impurity metrics. Two widely used ones are:
                <ul class="fade-in">
                    <li><strong>Entropy</strong> Measures uncertainty or disorder in the data.</li>
                    <li><strong>Gini Index:</strong> Measures the probability of incorrectly classifying a random element.</li>
                </ul>
            </p>

            <div class="image-container">
                <img src="assets/impurity_metrics.jpeg" alt="Impurity Metrics" class="topic-image">
            </div>

            <p class="fade-in">
                <strong>Information Gain</strong> is the reduction in impurity after a split:
                <br>
                <code>Information Gain = Impurity (Parent Node) ‚àí Weighted Average Impurity (Children Nodes)</code>
            </p>

            <p class="fade-in">
                <strong>Example:</strong> Suppose we have a dataset of 10 food items: 5 "Healthy" and 5 "Unhealthy."  
                Entropy before any split is:  
                $H = -0.5\log_2(0.5) - 0.5\log_2(0.5) = 1$  
                A split based on ‚ÄúCalories‚Äù results in subsets with entropy of 0.7 and 0.3 respectively.  
                Information Gain = 1 ‚àí (0.5 √ó 0.7 + 0.5 √ó 0.3) = 1 ‚àí 0.5 = **0.5**  
                <br>
                This shows that the split improved class purity and should be selected.
            </p>

            <p class="fade-in">
                Since features can continue splitting data with slightly better information gain, it is theoretically possible to generate infinite trees. This is why Decision Trees must be pruned or constrained (e.g., with max depth, min samples per leaf) to ensure generalization and prevent overfitting.
            </p>

            <!-- (b) Data Preparation -->
            <h2>Data Preparation</h2>
            <p class="fade-in">
                We used the same cleaned and preprocessed nutritional dataset from Multinomial Na√Øve Bayes for consistency. The feature matrices were taken from <code>nb_MNB_X_train.csv</code> and <code>nb_MNB_X_test.csv</code>, while the corresponding labels were pulled from <code>nb_GNB_y_train.csv</code> and <code>nb_GNB_y_test.csv</code>.
            </p>

            <p class="fade-in">
                A 70:30 stratified train-test split was applied to maintain proportional class distribution across both sets. Ensuring these sets are disjoint is critical to avoid data leakage‚Äîwhere knowledge of the test set contaminates the training process, leading to inflated accuracy and poor real-world generalization.
            </p>

            <!-- X_train -->
            <h4 class="fade-in">üìÅ X_train: Feature matrix for training</h4>
            <div class="image-container">
                <img src="assets/MNB_X_Train.jpg" alt="Decision Tree X_train" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_MNB_X_train.csv" target="_blank" class="github-button">View X_train CSV</a>
            </div>

            <!-- y_train -->
            <h4 class="fade-in">üè∑Ô∏è y_train: Labels for training</h4>
            <div class="image-container">
                <img src="assets/GNB_Y_Train.jpg" alt="Decision Tree y_train" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_GNB_y_train.csv" target="_blank" class="github-button">View y_train CSV</a>
            </div>

            <!-- X_test -->
            <h4 class="fade-in">üìÅ X_test: Feature matrix for evaluation</h4>
            <div class="image-container">
                <img src="assets/MNB_X_Test.jpg" alt="Decision Tree X_test" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_MNB_X_test.csv" target="_blank" class="github-button">View X_test CSV</a>
            </div>

            <!-- y_test -->
            <h4 class="fade-in">üè∑Ô∏è y_test: Ground truth labels for evaluation</h4>
            <div class="image-container">
                <img src="assets/GNB_Y_Test.jpg" alt="Decision Tree y_test" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_GNB_y_test.csv" target="_blank" class="github-button">View y_test CSV</a>
            </div>

            <!-- (c) Code -->
            <h2>Model Codebase</h2>
            <p class="fade-in">
                The Decision Tree models were implemented using Scikit-learn‚Äôs <code>DecisionTreeClassifier</code>. Three separate trees were trained using different root nodes by customizing <code>max_features</code> or manually selecting feature importance to control splitting. Evaluation was performed on the test set.
            </p>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/dt_model.py" target="_blank" class="github-button">View DT Model Code</a>
            </div>

            <!-- (d) Results -->
            <h2>Results and Visualizations</h2>
            <p class="fade-in">
                This section presents classification results using multiple variants of the Decision Tree model. The analysis explores how different nutrient-related root splits‚Äînamely <strong>calories</strong>, <strong>protein</strong>, and <strong>fat</strong>‚Äîimpact the prediction of food categories. A normalized confusion matrix and accuracy visuals are also included to support the model evaluation.
            </p>

            <h3 class="fade-in">Normalized Confusion Matrix</h3>
            <p class="fade-in">
                The matrix below shows the model‚Äôs performance on the top-10 most active food categories. Rows represent the true labels and columns the predicted ones. By normalizing values, we can better compare performance across classes.
            </p>

            <div class="image-container">
                <img src="visuals/confusion_matrix_simplified_top10_active.png" alt="Decision Tree Confusion Matrix" class="topic-image">
            </div>

            <p class="fade-in">
                The model demonstrates strong accuracy for categories like <strong>Soda</strong>, <strong>Pizza</strong>, and <strong>Ice Cream & Frozen Yogurt</strong>, with over 90% correct predictions. However, overlapping nutrient profiles lead to confusion between <strong>Cookies & Biscuits</strong>, <strong>Chocolate</strong>, and <strong>Chips & Pretzels</strong>. These confusions suggest a need for finer-grained features or ensemble methods in future modeling.
            </p>

            <h3 class="fade-in">Tree Variant 1 ‚Äì Root: <code>calories</code></h3>
            <p class="fade-in">
                The first decision tree variant uses <code>calories</code> as its root feature. It divides the data into low- and high-calorie items, leading to a dominant first split for <strong>Soda</strong>. Subsequent branches focus on <code>protein</code> to separate categories like <strong>Frozen Patties & Burgers</strong> and <strong>Ice Cream</strong>. This tree is the most balanced in terms of generalization and interpretability.
            </p>

            <div class="image-container">
                <img src="visuals/tree_variant1_simplified.png" alt="Tree Variant 1 - Root: Calories" class="topic-image">
            </div>

            <h3 class="fade-in">Tree Variant 2 ‚Äì Root: <code>protein</code></h3>
            <p class="fade-in">
                This variant begins with <code>protein</code> as the root node, isolating high-protein and low-protein foods early. Notably, <strong>Candy</strong> and <strong>Soda</strong> are separated efficiently at shallow depths due to their distinct low-protein content. However, this tree becomes more complex in mid-level branches and may be prone to misclassifying high-carb desserts such as <strong>Cookies</strong> and <strong>Chocolate</strong>.
            </p>

            <div class="image-container">
                <img src="visuals/tree_variant2_simplified.png" alt="Tree Variant 2 - Root: Protein" class="topic-image">
            </div>

            <h3 class="fade-in">Tree Variant 3 ‚Äì Root: <code>fat</code></h3>
            <p class="fade-in">
                The third variant explores <code>fat</code> as the root splitting criterion. This leads to early separation of <strong>Breads & Buns</strong> and <strong>Cookies & Biscuits</strong>. Although visually deeper, the tree reveals clearer clustering for <strong>Pizza</strong> and <strong>Candy</strong> in later splits. This variant offers a unique view into how fat-related nutrient structure contributes to classification.
            </p>

            <div class="image-container">
                <img src="visuals/tree_variant3_simplified.png" alt="Tree Variant 3 - Root: Fat" class="topic-image">
            </div>

            <h3 class="fade-in">Model Accuracy Snapshots</h3>
            <p class="fade-in">
                Below are final accuracy snapshots for each Decision Tree variant trained using the top-10 most frequent categories in the dataset.
                Each model varies in terms of its splitting strategy or root feature selection. The classification reports highlight class-wise precision, recall, and F1-scores.
            </p>
            
            <!-- Variant 1 -->
            <div class="image-container">
                <img src="assets/tree1_accuracy.png" alt="Tree Variant 1 Accuracy" class="topic-image">
            </div>
            <p class="fade-in">
                <strong>Tree Variant 1</strong> employed a Gini-based criterion with a shallow depth and fewer splits, making it the simplest of the three trees.
                This model achieved an overall accuracy of <strong>45.01%</strong>, with outstanding performance in identifying <em>Soda</em> (Precision: 0.95, Recall: 0.99) and <em>Cookies & Biscuits</em>.
                However, categories like <em>Pizza</em>, <em>Chips, Pretzels & Snacks</em>, and <em>Chocolate</em> had extremely poor recall and were often misclassified.
                This model seems to favor high-frequency, high-contrast categories while ignoring those with ambiguous features.
                The macro average F1-score is just <strong>0.22</strong>, reflecting low generalizability.
                Despite its simplicity, this model shows how tree-based models can struggle when class imbalance and feature similarity are high.
                Tree Variant 1 is good for interpretability but not optimal for high accuracy.
                It demonstrates how a dominant class like Soda can influence decision boundaries disproportionately.
            </p>
            
            <!-- Variant 2 -->
            <div class="image-container">
                <img src="assets/tree2_accuracy.png" alt="Tree Variant 2 Accuracy" class="topic-image">
            </div>
            <p class="fade-in">
                <strong>Tree Variant 2</strong> uses <code>entropy</code> as the splitting criterion, along with a slightly deeper structure and stricter leaf size conditions.
                This change led to a significant boost in performance, achieving <strong>62.26%</strong> overall accuracy.
                Categories like <em>Candy</em>, <em>Ice Cream & Frozen Yogurt</em>, <em>Cookies & Biscuits</em>, and <em>Soda</em> were predicted well.
                Precision for <em>Candy</em> was 0.88 and recall was 0.61, indicating a reliable but not perfect boundary.
                However, <em>Pizza</em> and <em>Chips, Pretzels & Snacks</em> still faced misclassification issues.
                The macro average F1-score increased to <strong>0.42</strong>, and the weighted average reached <strong>0.55</strong>, showing improved model stability.
                This variant shows the value of entropy-based information gain in dealing with class overlap.
                It balances performance across more classes than Variant 1, making it a better fit for datasets with moderate complexity.
            </p>
            
            <!-- Variant 3 -->
            <div class="image-container">
                <img src="assets/tree3_accuracy.png" alt="Tree Variant 3 Accuracy" class="topic-image">
            </div>
            <p class="fade-in">
                <strong>Tree Variant 3</strong> incorporates feature restriction (<code>max_features=2</code>) to increase diversity and prevent overfitting.
                It achieved a respectable <strong>52.20%</strong> accuracy, better than the Gini-based variant but lower than the entropy model.
                Notably, <em>Breads & Buns</em> had high precision (0.83) and recall (0.63), and <em>Candy</em> also performed well (Precision: 0.85, Recall: 0.68).
                The model struggled with <em>Cookies & Biscuits</em>, showing 0 values across all metrics ‚Äî likely due to being overshadowed by similar categories.
                This variant also improves generalizability while sacrificing some recall for minority classes.
                The macro F1-score stood at <strong>0.36</strong>, and the weighted F1 was <strong>0.43</strong>.
                It‚Äôs a great example of how controlled randomness in feature selection can reduce overfitting but needs fine-tuning for nuanced classes.
                Tree Variant 3 gives an alternative look into how diversity in decision paths impacts model outcomes.
            </p>


            <!-- (e) Conclusions -->
            <h2>Conclusions & Takeaways</h2>
            <ul class="fade-in">
                <li>
                    Decision Trees provide clear interpretability and are easy to implement on structured datasets like food nutrition.
                </li>
                <li>
                    Entropy and Gini both worked well, but Entropy-based splits yielded higher Information Gain for our dataset.
                </li>
                <li>
                    Tree depth control is vital; unregulated trees tend to overfit. Simpler, pruned trees had better generalization.
                </li>
                <li>
                    Root node selection significantly influenced classification accuracy, with "Calories" and "Fat" yielding the most distinct branches.
                </li>
            </ul>

            <p class="fade-in">
                Overall, Decision Tree modeling helped uncover key nutrient features that contribute most to food category classification, offering both prediction capability and nutritional insight.
            </p>

        </div>
    </div>


           <!-- Your existing HTML content -->

           <script>
            document.addEventListener("DOMContentLoaded", function() {
                let text = "Decision Trees";
                let i = 0;
                function typeWriter() {
                    if (i < text.length) {
                        document.getElementById("typing-title").innerHTML += text.charAt(i);
                        i++;
                        setTimeout(typeWriter, 50);
                    }
                }
                typeWriter();
            });
        </script>


        <!-- üîπ JavaScript for Scroll-Based Fade-In Effect -->
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                let fadeInElements = document.querySelectorAll(".fade-in");
    
                function fadeInOnScroll() {
                    fadeInElements.forEach(element => {
                        let position = element.getBoundingClientRect().top;
                        let screenHeight = window.innerHeight;
    
                        if (position < screenHeight - 100) {
                            element.classList.add("visible");
                        }
                    });
                }
    
                window.addEventListener("scroll", fadeInOnScroll);
                fadeInOnScroll(); // Trigger on load
            });
        </script>

    <script>
        function toggleMenu() {
            var navbar = document.querySelector(".navbar");
            navbar.classList.toggle("active");
        }
    </script>
    <script type="module" src="animations.js"></script>
    <div class="transition-overlay"></div>
    <script type="module" src="animations.js"></script>
</body>
</html>