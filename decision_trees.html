<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Decision Trees</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">
    <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
</head>
<body>

   <!-- Elegant Animated Background -->
   <div class="food-background"></div>
   <div class="light-overlay"></div>

    <!-- üîπ Hamburger Menu Icon -->
    <div class="menu-icon" onclick="toggleMenu()">‚ò∞</div>
    <div class="navbar">
        <a href="index.html">Home</a>
        <a href="introduction.html">Introduction</a>
        <a href="data_prep.html">Data Prep</a>
        <a href="eda.html">EDA</a>        
        <a href="pca.html">PCA</a>
        <a href="clustering.html">Clustering</a>
        <a href="arm.html">ARM</a>
        <a href="decision_trees.html"class="active">Decision Trees</a>
        <a href="naive_bayes.html">Naive Bayes</a>
        <a href="svm.html">SVM</a>
        <a href="regression.html">Regression</a>
        <a href="conclusions.html">Conclusions</a>
        <a href="about_me.html">About Me</a>
    </div>

    <div class="content-section">
        <h1 id="typing-title"></h1>
        <div class="content-wrapper">
            
            <!-- (a) Overview -->
            <h2>Introduction to Decision Trees for Food Classification</h2>

            <p class="fade-in">
                Decision Trees (DTs) are one of the most foundational and interpretable machine learning models. They operate by recursively partitioning the input data into smaller subsets based on feature values, ultimately forming a tree-like graph. Each internal node corresponds to a feature-based decision rule, each branch corresponds to the outcome of that rule, and each leaf node represents a final prediction‚Äîoften a class label in classification tasks or a numerical value in regression tasks.
            </p>

            <p class="fade-in">
                These models are used across a variety of domains such as medical diagnosis, fraud detection, and of course, nutritional analysis‚Äîwhere the relationships between features like calories, fat, sugar, and sodium can help predict food categories or healthiness levels. Decision Trees are favored because they are easy to visualize and interpret. Unlike black-box models, DTs allow us to track every decision and understand why a certain prediction was made.
            </p>

            <p class="fade-in">
                However, this flexibility comes at a cost. Decision Trees are prone to overfitting, especially if allowed to grow indefinitely. This is why regularization parameters such as <code>max_depth</code>, <code>min_samples_split</code>, and <code>min_samples_leaf</code> are essential‚Äîthey prevent the tree from memorizing the training data and failing to generalize to unseen samples.
            </p>

            <div class="image-container">
                <img src="assets/DT_structure.png" alt="Decision Tree Structure" class="topic-image">
            </div>

            <div class="image-container">
                <img src="assets/infogain.jpg" alt="Information Gain Illustration" class="topic-image">
            </div>

            <p class="fade-in">
                <strong>Impurity Metrics:</strong> At the core of decision tree construction lies the concept of impurity, which measures the degree of heterogeneity in a node. Two of the most popular impurity measures are:
            </p>

            <ul class="fade-in">
                <li><strong>Entropy:</strong> A concept borrowed from information theory, entropy measures the unpredictability or impurity in a dataset.</li>
                <div class="math-block">\[ H = - \sum p(x) \log_2 p(x) \]</div>
                <li><strong>Gini Index:</strong> A measure of impurity that evaluates the probability of a misclassification.</li>
                <div class="math-block">\[ G = 1 - \sum p(x)^2 \]</div>
            </ul>

            <div class="image-container">
                <img src="assets/impurity_metrics.jpeg" alt="Impurity Metrics" class="topic-image">
            </div>

            <p class="fade-in">
                <strong>Information Gain</strong> quantifies the effectiveness of a split by measuring the reduction in impurity. If splitting a node leads to purer subsets, that split is considered valuable. The formula is:
            </p>

            <div class="math-block">
                \[ \text{Information Gain} = \text{Impurity(Parent)} - \text{Weighted Avg Impurity(Children)} \]
            </div>

            <p class="fade-in">
                <strong>Illustrative Example:</strong> Consider a dataset with 10 food items: 5 are labeled as "Healthy" and 5 as "Unhealthy". The entropy at the root node is:
            </p>

            <div class="math-block">
                \[ H = -0.5 \log_2(0.5) - 0.5 \log_2(0.5) = 1 \]
            </div>

            <p class="fade-in">
                Now, imagine splitting based on the feature "Calories" yields two subsets: one with 4 Healthy and 1 Unhealthy (entropy = 0.72), and one with 1 Healthy and 4 Unhealthy (entropy = 0.72). Weighted average entropy of children = 0.72.
            </p>

            <div class="math-block">
                \[ \text{Information Gain} = 1 - 0.72 = 0.28 \]
            </div>

            <p class="fade-in">
                This means the split reduced impurity and added predictive power.
            </p>

            <p class="fade-in">
                It's also important to understand that Decision Trees can grow indefinitely. At every level, the algorithm tries to further split the data to gain slightly more information. If unchecked, it could result in a tree with one leaf per training example. This is known as overfitting. To avoid it, tree growth is constrained using pruning or limiting parameters like <code>max_depth</code>. Thus, although infinite trees are possible, they‚Äôre rarely desirable.
            </p>

            <p class="fade-in">
                In food classification tasks, Decision Trees serve as a powerful baseline because they reflect intuitive patterns in nutrition. For example, high sugar and low protein might signal Candy or Soda, while moderate calories and high fat could point to Pizza or Burgers. These models allow for both interpretability and feature-based hypothesis generation, making them a valuable tool in exploratory and predictive food analysis.
            </p>

            <!-- (b) Data Preparation -->
            <h2>Data Preparation</h2>
            <p class="fade-in">
                We used the same cleaned and preprocessed nutritional dataset from Multinomial Na√Øve Bayes for consistency. The feature matrices were taken from <code>nb_MNB_X_train.csv</code> and <code>nb_MNB_X_test.csv</code>, while the corresponding labels were pulled from <code>nb_GNB_y_train.csv</code> and <code>nb_GNB_y_test.csv</code>.
            </p>

            <p class="fade-in">
                A 70:30 stratified train-test split was applied to maintain proportional class distribution across both sets. Ensuring these sets are disjoint is critical to avoid data leakage‚Äîwhere knowledge of the test set contaminates the training process, leading to inflated accuracy and poor real-world generalization.
            </p>

            <!-- X_train -->
            <h4 class="fade-in">üìÅ X_train: Feature matrix for training</h4>
            <div class="image-container">
                <img src="assets/MNB_X_Train.jpg" alt="Decision Tree X_train" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_MNB_X_train.csv" target="_blank" class="github-button">View X_train CSV</a>
            </div>

            <!-- y_train -->
            <h4 class="fade-in">üè∑Ô∏è y_train: Labels for training</h4>
            <div class="image-container">
                <img src="assets/GNB_Y_Train.jpg" alt="Decision Tree y_train" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_GNB_y_train.csv" target="_blank" class="github-button">View y_train CSV</a>
            </div>

            <!-- X_test -->
            <h4 class="fade-in">üìÅ X_test: Feature matrix for evaluation</h4>
            <div class="image-container">
                <img src="assets/MNB_X_Test.jpg" alt="Decision Tree X_test" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_MNB_X_test.csv" target="_blank" class="github-button">View X_test CSV</a>
            </div>

            <!-- y_test -->
            <h4 class="fade-in">üè∑Ô∏è y_test: Ground truth labels for evaluation</h4>
            <div class="image-container">
                <img src="assets/GNB_Y_Test.jpg" alt="Decision Tree y_test" class="topic-image">
            </div>
            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/data/nb_GNB_y_test.csv" target="_blank" class="github-button">View y_test CSV</a>
            </div>

            <!-- (c) Code -->
            <h2>Model Codebase</h2>
            <p class="fade-in">
                The Decision Tree models were implemented using Scikit-learn‚Äôs <code>DecisionTreeClassifier</code>. Three separate trees were trained using different root nodes by customizing <code>max_features</code> or manually selecting feature importance to control splitting. Evaluation was performed on the test set.
            </p>

            <div class="button-section">
                <a href="https://github.com/mrxxnal/USDA_FOODS/blob/main/dt_model.py" target="_blank" class="github-button">View DT Model Code</a>
            </div>

            <!-- (d) Results -->
            <h2>Results and Visualizations</h2>
            <p class="fade-in">
                This section presents classification results using multiple variants of the Decision Tree model. The analysis explores how different nutrient-related root splits‚Äînamely <strong>calories</strong>, <strong>protein</strong>, and <strong>fat</strong>‚Äîimpact the prediction of food categories. A normalized confusion matrix and accuracy visuals are also included to support the model evaluation.
            </p>

            <h3 class="fade-in">Normalized Confusion Matrix</h3>
            <p class="fade-in">
                The matrix below shows the model‚Äôs performance on the top-10 most active food categories. Rows represent the true labels and columns the predicted ones. By normalizing values, we can better compare performance across classes.
            </p>

            <div class="image-container">
                <img src="visuals/confusion_matrix_simplified_top10_active.png" alt="Decision Tree Confusion Matrix" class="topic-image">
            </div>

            <p class="fade-in">
                The model demonstrates strong accuracy for categories like <strong>Soda</strong>, <strong>Pizza</strong>, and <strong>Ice Cream & Frozen Yogurt</strong>, with over 90% correct predictions. However, overlapping nutrient profiles lead to confusion between <strong>Cookies & Biscuits</strong>, <strong>Chocolate</strong>, and <strong>Chips & Pretzels</strong>. These confusions suggest a need for finer-grained features or ensemble methods in future modeling.
            </p>

            <h3 class="fade-in">Tree Variant 1 ‚Äì Root: <code>calories</code></h3>
            <p class="fade-in">
                The first decision tree variant uses <code>calories</code> as its root feature. It divides the data into low- and high-calorie items, leading to a dominant first split for <strong>Soda</strong>. Subsequent branches focus on <code>protein</code> to separate categories like <strong>Frozen Patties & Burgers</strong> and <strong>Ice Cream</strong>. This tree is the most balanced in terms of generalization and interpretability.
            </p>

            <div class="image-container">
                <img src="visuals/tree_variant1_simplified.png" alt="Tree Variant 1 - Root: Calories" class="topic-image">
            </div>

            <h3 class="fade-in">Tree Variant 2 ‚Äì Root: <code>protein</code></h3>
            <p class="fade-in">
                This variant begins with <code>protein</code> as the root node, isolating high-protein and low-protein foods early. Notably, <strong>Candy</strong> and <strong>Soda</strong> are separated efficiently at shallow depths due to their distinct low-protein content. However, this tree becomes more complex in mid-level branches and may be prone to misclassifying high-carb desserts such as <strong>Cookies</strong> and <strong>Chocolate</strong>.
            </p>

            <div class="image-container">
                <img src="visuals/tree_variant2_simplified.png" alt="Tree Variant 2 - Root: Protein" class="topic-image">
            </div>

            <h3 class="fade-in">Tree Variant 3 ‚Äì Root: <code>fat</code></h3>
            <p class="fade-in">
                The third variant explores <code>fat</code> as the root splitting criterion. This leads to early separation of <strong>Breads & Buns</strong> and <strong>Cookies & Biscuits</strong>. Although visually deeper, the tree reveals clearer clustering for <strong>Pizza</strong> and <strong>Candy</strong> in later splits. This variant offers a unique view into how fat-related nutrient structure contributes to classification.
            </p>

            <div class="image-container">
                <img src="visuals/tree_variant3_simplified.png" alt="Tree Variant 3 - Root: Fat" class="topic-image">
            </div>

            <h3 class="fade-in">Model Accuracy Snapshots</h3>
            <p class="fade-in">
                Below are final accuracy snapshots for each Decision Tree variant trained using the top-10 most frequent categories in the dataset.
                Each model varies in terms of its splitting strategy or root feature selection. The classification reports highlight class-wise precision, recall, and F1-scores.
            </p>
            
            <!-- Variant 1 -->
            <div class="image-container">
                <img src="assets/tree1_accuracy.png" alt="Tree Variant 1 Accuracy" class="topic-image">
            </div>
            <p class="fade-in">
                <strong>Tree Variant 1</strong> employed a Gini-based criterion with a shallow depth and fewer splits, making it the simplest of the three trees.
                This model achieved an overall accuracy of <strong>45.01%</strong>, with outstanding performance in identifying <em>Soda</em> (Precision: 0.95, Recall: 0.99) and <em>Cookies & Biscuits</em>.
                However, categories like <em>Pizza</em>, <em>Chips, Pretzels & Snacks</em>, and <em>Chocolate</em> had extremely poor recall and were often misclassified.
                This model seems to favor high-frequency, high-contrast categories while ignoring those with ambiguous features.
                The macro average F1-score is just <strong>0.22</strong>, reflecting low generalizability.
                Despite its simplicity, this model shows how tree-based models can struggle when class imbalance and feature similarity are high.
                Tree Variant 1 is good for interpretability but not optimal for high accuracy.
                It demonstrates how a dominant class like Soda can influence decision boundaries disproportionately.
            </p>
            
            <!-- Variant 2 -->
            <div class="image-container">
                <img src="assets/tree2_accuracy.png" alt="Tree Variant 2 Accuracy" class="topic-image">
            </div>
            <p class="fade-in">
                <strong>Tree Variant 2</strong> uses <code>entropy</code> as the splitting criterion, along with a slightly deeper structure and stricter leaf size conditions.
                This change led to a significant boost in performance, achieving <strong>62.26%</strong> overall accuracy.
                Categories like <em>Candy</em>, <em>Ice Cream & Frozen Yogurt</em>, <em>Cookies & Biscuits</em>, and <em>Soda</em> were predicted well.
                Precision for <em>Candy</em> was 0.88 and recall was 0.61, indicating a reliable but not perfect boundary.
                However, <em>Pizza</em> and <em>Chips, Pretzels & Snacks</em> still faced misclassification issues.
                The macro average F1-score increased to <strong>0.42</strong>, and the weighted average reached <strong>0.55</strong>, showing improved model stability.
                This variant shows the value of entropy-based information gain in dealing with class overlap.
                It balances performance across more classes than Variant 1, making it a better fit for datasets with moderate complexity.
            </p>
            
            <!-- Variant 3 -->
            <div class="image-container">
                <img src="assets/tree3_accuracy.png" alt="Tree Variant 3 Accuracy" class="topic-image">
            </div>
            <p class="fade-in">
                <strong>Tree Variant 3</strong> incorporates feature restriction (<code>max_features=2</code>) to increase diversity and prevent overfitting.
                It achieved a respectable <strong>52.20%</strong> accuracy, better than the Gini-based variant but lower than the entropy model.
                Notably, <em>Breads & Buns</em> had high precision (0.83) and recall (0.63), and <em>Candy</em> also performed well (Precision: 0.85, Recall: 0.68).
                The model struggled with <em>Cookies & Biscuits</em>, showing 0 values across all metrics ‚Äî likely due to being overshadowed by similar categories.
                This variant also improves generalizability while sacrificing some recall for minority classes.
                The macro F1-score stood at <strong>0.36</strong>, and the weighted F1 was <strong>0.43</strong>.
                It‚Äôs a great example of how controlled randomness in feature selection can reduce overfitting but needs fine-tuning for nuanced classes.
                Tree Variant 3 gives an alternative look into how diversity in decision paths impacts model outcomes.
            </p>


            <!-- (e) Conclusions -->
            <h2>Conclusions & Takeaways</h2>
            <ul class="fade-in">
                <li>
                    Decision Trees provide clear interpretability and are easy to implement on structured datasets like food nutrition.
                </li>
                <li>
                    Entropy and Gini both worked well, but Entropy-based splits yielded higher Information Gain for our dataset.
                </li>
                <li>
                    Tree depth control is vital; unregulated trees tend to overfit. Simpler, pruned trees had better generalization.
                </li>
                <li>
                    Root node selection significantly influenced classification accuracy, with "Calories" and "Fat" yielding the most distinct branches.
                </li>
            </ul>

            <p class="fade-in">
                Overall, Decision Tree modeling helped uncover key nutrient features that contribute most to food category classification, offering both prediction capability and nutritional insight.
            </p>

        </div>
    </div>


           <!-- Your existing HTML content -->

           <script>
            document.addEventListener("DOMContentLoaded", function() {
                let text = "Decision Trees";
                let i = 0;
                function typeWriter() {
                    if (i < text.length) {
                        document.getElementById("typing-title").innerHTML += text.charAt(i);
                        i++;
                        setTimeout(typeWriter, 50);
                    }
                }
                typeWriter();
            });
        </script>


        <!-- üîπ JavaScript for Scroll-Based Fade-In Effect -->
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                let fadeInElements = document.querySelectorAll(".fade-in");
    
                function fadeInOnScroll() {
                    fadeInElements.forEach(element => {
                        let position = element.getBoundingClientRect().top;
                        let screenHeight = window.innerHeight;
    
                        if (position < screenHeight - 100) {
                            element.classList.add("visible");
                        }
                    });
                }
    
                window.addEventListener("scroll", fadeInOnScroll);
                fadeInOnScroll(); // Trigger on load
            });
        </script>

    <script>
        function toggleMenu() {
            var navbar = document.querySelector(".navbar");
            navbar.classList.toggle("active");
        }
    </script>
    <script type="module" src="animations.js"></script>
    <div class="transition-overlay"></div>
    <script type="module" src="animations.js"></script>
</body>
</html>